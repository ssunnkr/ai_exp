{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 프로젝트 : 인공지능과 가위바위보 하기\n",
    "##### !. VScode 플랫폼을 사용하여 가상환경 Python 3.9.7버전에서 작성되었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.0. 라이브러리 import 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "1.22.3\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지를 제공해주신 경문님, 창환님, 범석님, 오연님, 형진님, 해창님, 예슬님, 세한님 감사드립니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사진의 크기를 224x224에서 28x28로 변경\n",
    "def resize_images(img_path):\n",
    "\timages = glob.glob(img_path+\"/*.jpg\")\n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "\t# 각 이미지 사이즈 변경\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Image.show of <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7FDE80431EE0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디렉토리에 특수문자가 들어가면 os 및 glob기능들이 제대로 쓰이지 않으니 조심할것.\n",
    "#ex #image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/[E_05]Sequential/rock_scissor_paper/paper/1200.jpeg\"\n",
    "#print(image_dir_path) #경로 확인용\n",
    "# 이미지 확인용 이미지 불러오기\n",
    "mmm = Image.open(os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/paper/1200.jpg\")\n",
    "#mmm = Image.open(\"/Users/hpcd/Desktop/Code/LMS/E_05_Sequential/scissor/1.jpg\")\n",
    "# 이미지 불러오기.\n",
    "mmm.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329  images to be resized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/djd_zrlx0kg8fhjfrqdvzsg00000gq/T/ipykernel_17281/3293707581.py:10: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  new_img=old_img.resize(target_size,Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "1120  images to be resized.\n",
      "1120  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "1211  images to be resized.\n",
      "1211  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"보 이미지 resize 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (본문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라벨링 및 데이터 스플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 3660 입니다.\n",
      "x_train shape: (3660, 28, 28, 3)\n",
      "y_train shape: (3660,)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# 이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성\n",
    "def load_data(img_path, number_of_data=3660):  # 가위바위보 이미지 개수 총합에 주의.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28 # resize할 크기\n",
    "    color=3 # 컬러는 3, 흑백은 1\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "# 이미지 불러오기\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 네트워크 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 20)        560       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 13, 13, 20)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 11, 11, 40)        7240      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 5, 5, 40)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                30030     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 93        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,923\n",
      "Trainable params: 37,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#import numpy as np\n",
    "# tf.keras의 Sequential API를 이용하여 LeNet이라는 딥러닝 네트워크를 설계\n",
    "# 정확도를 높이기 위해 변경할 수 있는 하이퍼파라미터들\n",
    "# n_epoch값도 변경 가능하나 너무 높아지면 overfitting 문제가 발생 하므로 건드리지 않는게 좋음\n",
    "n_channel_1=20\n",
    "n_channel_2=40\n",
    "n_dense=30\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "# input_shape=(이미지 높이,이미지 너비,채널 수) # 채널 수 흑백이면 1, 컬러면 3으로 설정\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))  # 히든 레이어\n",
    "\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))  # 종속 변수 컬럼 3\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "# 만든 딥러닝 네트워크 모델 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "293/293 [==============================] - 5s 13ms/step - loss: 0.8587 - accuracy: 0.5963 - val_loss: 1.0138 - val_accuracy: 0.3784\n",
      "Epoch 2/20\n",
      "293/293 [==============================] - 4s 14ms/step - loss: 0.5136 - accuracy: 0.7927 - val_loss: 1.4918 - val_accuracy: 0.3238\n",
      "Epoch 3/20\n",
      "293/293 [==============================] - 5s 15ms/step - loss: 0.2960 - accuracy: 0.8856 - val_loss: 1.6071 - val_accuracy: 0.4126\n",
      "Epoch 4/20\n",
      "293/293 [==============================] - 6s 20ms/step - loss: 0.2222 - accuracy: 0.9150 - val_loss: 1.0704 - val_accuracy: 0.6598\n",
      "Epoch 5/20\n",
      "293/293 [==============================] - 7s 24ms/step - loss: 0.1619 - accuracy: 0.9457 - val_loss: 0.8362 - val_accuracy: 0.7131\n",
      "Epoch 6/20\n",
      "293/293 [==============================] - 9s 30ms/step - loss: 0.1332 - accuracy: 0.9553 - val_loss: 0.4369 - val_accuracy: 0.8470\n",
      "Epoch 7/20\n",
      "293/293 [==============================] - 10s 34ms/step - loss: 0.0983 - accuracy: 0.9676 - val_loss: 0.7940 - val_accuracy: 0.7213\n",
      "Epoch 8/20\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0769 - accuracy: 0.9785 - val_loss: 0.5868 - val_accuracy: 0.7923\n",
      "Epoch 9/20\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0619 - accuracy: 0.9819 - val_loss: 0.4191 - val_accuracy: 0.8402\n",
      "Epoch 10/20\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0709 - accuracy: 0.9781 - val_loss: 0.7347 - val_accuracy: 0.7514\n",
      "Epoch 11/20\n",
      "293/293 [==============================] - 9s 32ms/step - loss: 0.0598 - accuracy: 0.9853 - val_loss: 0.5834 - val_accuracy: 0.8046\n",
      "Epoch 12/20\n",
      "293/293 [==============================] - 9s 30ms/step - loss: 0.0265 - accuracy: 0.9939 - val_loss: 1.0314 - val_accuracy: 0.7322\n",
      "Epoch 13/20\n",
      "293/293 [==============================] - 9s 29ms/step - loss: 0.0281 - accuracy: 0.9942 - val_loss: 0.3784 - val_accuracy: 0.8538\n",
      "Epoch 14/20\n",
      "293/293 [==============================] - 9s 32ms/step - loss: 0.0359 - accuracy: 0.9898 - val_loss: 1.0305 - val_accuracy: 0.7541\n",
      "Epoch 15/20\n",
      "293/293 [==============================] - 9s 29ms/step - loss: 0.0350 - accuracy: 0.9904 - val_loss: 0.4677 - val_accuracy: 0.8429\n",
      "Epoch 16/20\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.5442 - val_accuracy: 0.8265\n",
      "Epoch 17/20\n",
      "293/293 [==============================] - 9s 30ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.5044 - val_accuracy: 0.8484\n",
      "Epoch 18/20\n",
      "293/293 [==============================] - 9s 31ms/step - loss: 0.0321 - accuracy: 0.9918 - val_loss: 0.9439 - val_accuracy: 0.7760\n",
      "Epoch 19/20\n",
      "293/293 [==============================] - 8s 28ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 0.5583 - val_accuracy: 0.8415\n",
      "Epoch 20/20\n",
      "293/293 [==============================] - 8s 28ms/step - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.1353 - val_accuracy: 0.9590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde08fb7dc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped=x_train_norm.reshape(-1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됨.\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 학습(Fit)\n",
    "n_train_epoch = 20\n",
    "model.fit(x_train_reshaped, y_train, epochs=n_train_epoch, validation_split=0.2, batch_size=10)  # epochs=n # n번 반복해서 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329  images to be resized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/djd_zrlx0kg8fhjfrqdvzsg00000gq/T/ipykernel_17281/3293707581.py:10: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  new_img=old_img.resize(target_size,Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329  images resized.\n",
      "1120  images to be resized.\n",
      "1120  images resized.\n",
      "1211  images to be resized.\n",
      "1211  images resized.\n",
      "학습데이터(x_train)의 이미지 개수는 3660 입니다.\n",
      "x_test shape: (3660, 28, 28, 3)\n",
      "y_test shape: (3660,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test 생성\n",
    "# test 데이터는 오버피팅을 피하기 위해 사진을 교체해서 진행\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/Desktop/Code/LMS/E_05_Sequential/scissor_rock_paper/test\" \n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 - 2s - loss: 0.5051 - accuracy: 0.8598 - 2s/epoch - 15ms/step\n",
      "test_loss: 0.5051233768463135 \n",
      "test_accuracy: 0.8598360419273376\n"
     ]
    }
   ],
   "source": [
    "x_test_reshaped=x_test_norm.reshape(-1, 28, 28, 3)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n",
    "# accuracy 85% 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가기준."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 이미지 분류기 모델이 성공적으로 만들어졌는가?  \n",
    "트레이닝이 정상적으로 수행되었음  \n",
    "2. 오버피팅을 극복하기 위한 적절한 시도가 있었는가?  \n",
    "데이터셋의 다양성, 정규화 등의 시도가 적절하였음  \n",
    "3. 분류모델의 test accuracy가 기준 이상 높게 나왔는가?  \n",
    "60% 이상 도달하였음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고록."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 찍은 가위, 바위, 보 이미지를 활용해 적절한 데이터 전처리를 거쳐 분류모델을 만들기 위한 딥러닝 네트워크 설계및 학습을 통해 만들어진 나만의 모델을 통해 다른 사진들과 비교해 적절한 test accuracy를 뽑아내기 위해 여러 하이퍼파라미터 값들을 바꿔 모델의 정확도를 높이는 다섯번째 explation이었다.  \n",
    "지금까지 친절하게 a-z까지 잘 설명되어있던 앞선 exp와는 달리 설명 부분들이 조금씩 간소화된게 있어 초반 부분 이해하는데에 어려움을 겪었다.  \n",
    "코드를 찾아보면서 처음 모델을 만들었을 때는 accuracy가 95% 이상이 나와 너무 놀랐었다.  \n",
    "천천히 코드를 리뷰해보니 train data와 test data가 같았음을 깨달았고 이로 인해 오버피팅이 생겼음을 깨달았다.  \n",
    "다섯번째 exp 과제를 받은날에 해창퍼실님이 넌지시 말씀해주셨던 그 상황이었다.  \n",
    "다른 사진에서도 분류 모델이 제대로 작동하는지 확인하기 위해 테스트 사진을 바꾸자 accuracy가 굉장히 낮은 10%이하가 나왔다.  \n",
    "이 상황을 타개하기 위해 아무리 하이퍼 파라미터 값을 바꿔도 accuracy 값은 올라가지 않았기에 굉장히 막막했었다.  \n",
    "다행히도 같은 aiffel 식구분들이 많은 사진을 공유해주셨고 이 데이터를 활용해 대량의 사진을 집어넣었고 data가 많아지니 실제 모델 성능도 좋아져 accuracy가 굉장히 높아졌다.  \n",
    "다양한 환경에서 찍은 사진들이기에 빛, 각도, 가위의 모양 등이 달라 정확도를 100%까지 끌어올릴 수는 없었지만 그럼에도 exp에서 제시한 %를 넘길 수 있어서 기뻤다.   \n",
    "aiffel 식구분들께 이 자리를 빌어 감사의 인사를 올린다.  \n",
    "이번 실습을 통해 하이퍼 파라미터도 중요하지만 데이터의 물량도 중요함을 다시한번 깨달았다.  \n",
    "왜 딥러닝에서 데이터가 중요한지도 말이다.  \n",
    "\n",
    "이번 exp를 진행하기 위해 여러 코드를 찾던 중 실시간으로 움직이는 손모양을 분석하여 실시간으로 가위 바위 보를 인식하는 코드나 사람의 얼굴 표정을 인식해 감정을 구분해보는 코드들도 있었다.  \n",
    "CV로 나아가기 위해서는 이번 모델에 원리를 잘 이해하고 넘어가야 할 필요가 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference. (APA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 101 (https://www.youtube.com/playlist?list=PLl1irxoYh2wyLwJutUZx5Q_QEEDZoXBnz)  \n",
    "Tensorflow 102 - 이미지 분류(CNN) (https://www.youtube.com/playlist?list=PLl1irxoYh2wzOOU9hvJqMYc215wAlxrpp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a59dc1dcb280767e1541beb883352acbe24e77c028211ef192031d7df32c832b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
