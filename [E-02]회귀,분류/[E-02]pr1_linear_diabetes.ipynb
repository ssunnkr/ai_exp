{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 1 : 손수 설계하는 선형회귀, 당뇨병 수치를 맞춰보자!\n",
    "##### !. VScode 플랫폼을 사용하여 가상환경 Python 3.9.7버전에서 작성되었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 데이터 가져오기\n",
    "sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.  \n",
    "diabetes의 data를 df_X에, target을 df_y에 저장해주세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.3\n",
      "1.4.2\n",
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "print(np.__version__) # numpy version 1.22.3\n",
    "print(pd.__version__) # pands version 1.4.2\n",
    "print(sklearn.__version__) # sklearn version 1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error #mse 모듈을 사용하기 위해 불러옴\n",
    "import matplotlib.pyplot as plt #그래프 기능을 사용하기 위해 불러옴 (8번용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'data_filename', 'data_module', 'feature_names', 'frame', 'target', 'target_filename']\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "print(dir(diabetes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, total serum cholesterol\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, total cholesterol / HDL\\n      - s5      ltg, possibly log of serum triglycerides level\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,\n",
       "       -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = diabetes.data\n",
    "print(df_X.shape) # 442 10\n",
    "print(type(df_X)) # numpy.ndarray\n",
    "df_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y = diabetes.target\n",
    "print(df_y.shape)\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetes.target_filename # diabetes_target.csv.gz\n",
    "#diabetes.feature_names #['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df =pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "diabetes_df #442rows x 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  label  \n",
       "0   -0.002592  0.019908 -0.017646  151.0  \n",
       "1   -0.039493 -0.068330 -0.092204   75.0  \n",
       "2   -0.002592  0.002864 -0.025930  141.0  \n",
       "3    0.034309  0.022692 -0.009362  206.0  \n",
       "4   -0.002592 -0.031991 -0.046641  135.0  \n",
       "..        ...       ...       ...    ...  \n",
       "437 -0.002592  0.031193  0.007207  178.0  \n",
       "438  0.034309 -0.018118  0.044485  104.0  \n",
       "439 -0.011080 -0.046879  0.015491  132.0  \n",
       "440  0.026560  0.044528 -0.025930  220.0  \n",
       "441 -0.039493 -0.004220  0.003064   57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df[\"label\"] = diabetes.target\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델에 입력할 데이터 x 준비하기\n",
    "df_X에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 모델에 예측할 데이터 y 준비하기\n",
    "df_y에 있는 값들을 numpy array로 변환해서 저장해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) train 데이터와 test 데이터로 분리하기\n",
    "X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state 42가 train테스트와 test 데이터 결과성능이 유사하게 나옴.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# 10개의 정보(feature)가 있는 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.  \n",
    "모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy 필요\n",
    "W = np.random.rand(10)# 10개\n",
    "b = np.random.rand() # 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33857622, 0.21211308, 0.07906105, 0.16901643, 0.75043382,\n",
       "       0.65000693, 0.44607038, 0.04582096, 0.27136912, 0.79728454])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8725643462585487"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델함수 구현\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 손실함수 loss 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "기울기를 계산하는 gradient 함수를 구현해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 가중치의 개수\n",
    "    N = len(W)\n",
    "\n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "\n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "\n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [ -60.62176054  -13.84495687 -189.71289976 -142.74479549  -68.26350264\n",
      "  -56.00890414  127.79540428 -139.12296464 -182.9654649  -123.54521277]\n",
      "db: -302.52183963327474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndW: [ -60.53317007  -13.7179316  -189.63390588 -142.56862429  -68.30374602\\n  -56.05098535  127.68260888 -139.05192987 -182.86590269 -123.47677324]\\ndb: -302.73555424399063\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient 계산\n",
    "dW, db = gradient(X, W, b, y)\n",
    "print(\"dW:\", dW)\n",
    "print(\"db:\", db)\n",
    "\n",
    "'''\n",
    "dW: [ -60.53317007  -13.7179316  -189.63390588 -142.56862429  -68.30374602\n",
    "  -56.05098535  127.68260888 -139.05192987 -182.86590269 -123.47677324]\n",
    "db: -302.73555424399063\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "학습률, learning rate 를 설정해주세요  \n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 0.001\n",
    "LEARNNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.  \n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.  \n",
    "입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 2898.3420\n",
      "Iteration 20 : Loss 2898.3394\n",
      "Iteration 30 : Loss 2898.3369\n",
      "Iteration 40 : Loss 2898.3343\n",
      "Iteration 50 : Loss 2898.3317\n",
      "Iteration 60 : Loss 2898.3291\n",
      "Iteration 70 : Loss 2898.3265\n",
      "Iteration 80 : Loss 2898.3240\n",
      "Iteration 90 : Loss 2898.3214\n",
      "Iteration 100 : Loss 2898.3188\n",
      "Iteration 110 : Loss 2898.3163\n",
      "Iteration 120 : Loss 2898.3137\n",
      "Iteration 130 : Loss 2898.3112\n",
      "Iteration 140 : Loss 2898.3086\n",
      "Iteration 150 : Loss 2898.3061\n",
      "Iteration 160 : Loss 2898.3036\n",
      "Iteration 170 : Loss 2898.3010\n",
      "Iteration 180 : Loss 2898.2985\n",
      "Iteration 190 : Loss 2898.2960\n",
      "Iteration 200 : Loss 2898.2934\n",
      "Iteration 210 : Loss 2898.2909\n",
      "Iteration 220 : Loss 2898.2884\n",
      "Iteration 230 : Loss 2898.2859\n",
      "Iteration 240 : Loss 2898.2834\n",
      "Iteration 250 : Loss 2898.2809\n",
      "Iteration 260 : Loss 2898.2784\n",
      "Iteration 270 : Loss 2898.2759\n",
      "Iteration 280 : Loss 2898.2734\n",
      "Iteration 290 : Loss 2898.2709\n",
      "Iteration 300 : Loss 2898.2684\n",
      "Iteration 310 : Loss 2898.2659\n",
      "Iteration 320 : Loss 2898.2634\n",
      "Iteration 330 : Loss 2898.2609\n",
      "Iteration 340 : Loss 2898.2585\n",
      "Iteration 350 : Loss 2898.2560\n",
      "Iteration 360 : Loss 2898.2535\n",
      "Iteration 370 : Loss 2898.2511\n",
      "Iteration 380 : Loss 2898.2486\n",
      "Iteration 390 : Loss 2898.2461\n",
      "Iteration 400 : Loss 2898.2437\n",
      "Iteration 410 : Loss 2898.2412\n",
      "Iteration 420 : Loss 2898.2388\n",
      "Iteration 430 : Loss 2898.2363\n",
      "Iteration 440 : Loss 2898.2339\n",
      "Iteration 450 : Loss 2898.2315\n",
      "Iteration 460 : Loss 2898.2290\n",
      "Iteration 470 : Loss 2898.2266\n",
      "Iteration 480 : Loss 2898.2242\n",
      "Iteration 490 : Loss 2898.2217\n",
      "Iteration 500 : Loss 2898.2193\n",
      "Iteration 510 : Loss 2898.2169\n",
      "Iteration 520 : Loss 2898.2145\n",
      "Iteration 530 : Loss 2898.2121\n",
      "Iteration 540 : Loss 2898.2097\n",
      "Iteration 550 : Loss 2898.2073\n",
      "Iteration 560 : Loss 2898.2049\n",
      "Iteration 570 : Loss 2898.2025\n",
      "Iteration 580 : Loss 2898.2001\n",
      "Iteration 590 : Loss 2898.1977\n",
      "Iteration 600 : Loss 2898.1953\n",
      "Iteration 610 : Loss 2898.1929\n",
      "Iteration 620 : Loss 2898.1905\n",
      "Iteration 630 : Loss 2898.1882\n",
      "Iteration 640 : Loss 2898.1858\n",
      "Iteration 650 : Loss 2898.1834\n",
      "Iteration 660 : Loss 2898.1811\n",
      "Iteration 670 : Loss 2898.1787\n",
      "Iteration 680 : Loss 2898.1763\n",
      "Iteration 690 : Loss 2898.1740\n",
      "Iteration 700 : Loss 2898.1716\n",
      "Iteration 710 : Loss 2898.1693\n",
      "Iteration 720 : Loss 2898.1669\n",
      "Iteration 730 : Loss 2898.1646\n",
      "Iteration 740 : Loss 2898.1622\n",
      "Iteration 750 : Loss 2898.1599\n",
      "Iteration 760 : Loss 2898.1576\n",
      "Iteration 770 : Loss 2898.1552\n",
      "Iteration 780 : Loss 2898.1529\n",
      "Iteration 790 : Loss 2898.1506\n",
      "Iteration 800 : Loss 2898.1483\n",
      "Iteration 810 : Loss 2898.1459\n",
      "Iteration 820 : Loss 2898.1436\n",
      "Iteration 830 : Loss 2898.1413\n",
      "Iteration 840 : Loss 2898.1390\n",
      "Iteration 850 : Loss 2898.1367\n",
      "Iteration 860 : Loss 2898.1344\n",
      "Iteration 870 : Loss 2898.1321\n",
      "Iteration 880 : Loss 2898.1298\n",
      "Iteration 890 : Loss 2898.1275\n",
      "Iteration 900 : Loss 2898.1252\n",
      "Iteration 910 : Loss 2898.1229\n",
      "Iteration 920 : Loss 2898.1206\n",
      "Iteration 930 : Loss 2898.1184\n",
      "Iteration 940 : Loss 2898.1161\n",
      "Iteration 950 : Loss 2898.1138\n",
      "Iteration 960 : Loss 2898.1115\n",
      "Iteration 970 : Loss 2898.1093\n",
      "Iteration 980 : Loss 2898.1070\n",
      "Iteration 990 : Loss 2898.1047\n",
      "Iteration 1000 : Loss 2898.1025\n",
      "Iteration 1010 : Loss 2898.1002\n",
      "Iteration 1020 : Loss 2898.0980\n",
      "Iteration 1030 : Loss 2898.0957\n",
      "Iteration 1040 : Loss 2898.0935\n",
      "Iteration 1050 : Loss 2898.0912\n",
      "Iteration 1060 : Loss 2898.0890\n",
      "Iteration 1070 : Loss 2898.0867\n",
      "Iteration 1080 : Loss 2898.0845\n",
      "Iteration 1090 : Loss 2898.0823\n",
      "Iteration 1100 : Loss 2898.0800\n",
      "Iteration 1110 : Loss 2898.0778\n",
      "Iteration 1120 : Loss 2898.0756\n",
      "Iteration 1130 : Loss 2898.0734\n",
      "Iteration 1140 : Loss 2898.0712\n",
      "Iteration 1150 : Loss 2898.0689\n",
      "Iteration 1160 : Loss 2898.0667\n",
      "Iteration 1170 : Loss 2898.0645\n",
      "Iteration 1180 : Loss 2898.0623\n",
      "Iteration 1190 : Loss 2898.0601\n",
      "Iteration 1200 : Loss 2898.0579\n",
      "Iteration 1210 : Loss 2898.0557\n",
      "Iteration 1220 : Loss 2898.0535\n",
      "Iteration 1230 : Loss 2898.0513\n",
      "Iteration 1240 : Loss 2898.0491\n",
      "Iteration 1250 : Loss 2898.0469\n",
      "Iteration 1260 : Loss 2898.0448\n",
      "Iteration 1270 : Loss 2898.0426\n",
      "Iteration 1280 : Loss 2898.0404\n",
      "Iteration 1290 : Loss 2898.0382\n",
      "Iteration 1300 : Loss 2898.0360\n",
      "Iteration 1310 : Loss 2898.0339\n",
      "Iteration 1320 : Loss 2898.0317\n",
      "Iteration 1330 : Loss 2898.0295\n",
      "Iteration 1340 : Loss 2898.0274\n",
      "Iteration 1350 : Loss 2898.0252\n",
      "Iteration 1360 : Loss 2898.0231\n",
      "Iteration 1370 : Loss 2898.0209\n",
      "Iteration 1380 : Loss 2898.0188\n",
      "Iteration 1390 : Loss 2898.0166\n",
      "Iteration 1400 : Loss 2898.0145\n",
      "Iteration 1410 : Loss 2898.0123\n",
      "Iteration 1420 : Loss 2898.0102\n",
      "Iteration 1430 : Loss 2898.0081\n",
      "Iteration 1440 : Loss 2898.0059\n",
      "Iteration 1450 : Loss 2898.0038\n",
      "Iteration 1460 : Loss 2898.0017\n",
      "Iteration 1470 : Loss 2897.9995\n",
      "Iteration 1480 : Loss 2897.9974\n",
      "Iteration 1490 : Loss 2897.9953\n",
      "Iteration 1500 : Loss 2897.9932\n",
      "Iteration 1510 : Loss 2897.9911\n",
      "Iteration 1520 : Loss 2897.9889\n",
      "Iteration 1530 : Loss 2897.9868\n",
      "Iteration 1540 : Loss 2897.9847\n",
      "Iteration 1550 : Loss 2897.9826\n",
      "Iteration 1560 : Loss 2897.9805\n",
      "Iteration 1570 : Loss 2897.9784\n",
      "Iteration 1580 : Loss 2897.9763\n",
      "Iteration 1590 : Loss 2897.9742\n",
      "Iteration 1600 : Loss 2897.9721\n",
      "Iteration 1610 : Loss 2897.9701\n",
      "Iteration 1620 : Loss 2897.9680\n",
      "Iteration 1630 : Loss 2897.9659\n",
      "Iteration 1640 : Loss 2897.9638\n",
      "Iteration 1650 : Loss 2897.9617\n",
      "Iteration 1660 : Loss 2897.9597\n",
      "Iteration 1670 : Loss 2897.9576\n",
      "Iteration 1680 : Loss 2897.9555\n",
      "Iteration 1690 : Loss 2897.9534\n",
      "Iteration 1700 : Loss 2897.9514\n",
      "Iteration 1710 : Loss 2897.9493\n",
      "Iteration 1720 : Loss 2897.9473\n",
      "Iteration 1730 : Loss 2897.9452\n",
      "Iteration 1740 : Loss 2897.9431\n",
      "Iteration 1750 : Loss 2897.9411\n",
      "Iteration 1760 : Loss 2897.9390\n",
      "Iteration 1770 : Loss 2897.9370\n",
      "Iteration 1780 : Loss 2897.9350\n",
      "Iteration 1790 : Loss 2897.9329\n",
      "Iteration 1800 : Loss 2897.9309\n",
      "Iteration 1810 : Loss 2897.9288\n",
      "Iteration 1820 : Loss 2897.9268\n",
      "Iteration 1830 : Loss 2897.9248\n",
      "Iteration 1840 : Loss 2897.9227\n",
      "Iteration 1850 : Loss 2897.9207\n",
      "Iteration 1860 : Loss 2897.9187\n",
      "Iteration 1870 : Loss 2897.9167\n",
      "Iteration 1880 : Loss 2897.9146\n",
      "Iteration 1890 : Loss 2897.9126\n",
      "Iteration 1900 : Loss 2897.9106\n",
      "Iteration 1910 : Loss 2897.9086\n",
      "Iteration 1920 : Loss 2897.9066\n",
      "Iteration 1930 : Loss 2897.9046\n",
      "Iteration 1940 : Loss 2897.9026\n",
      "Iteration 1950 : Loss 2897.9006\n",
      "Iteration 1960 : Loss 2897.8986\n",
      "Iteration 1970 : Loss 2897.8966\n",
      "Iteration 1980 : Loss 2897.8946\n",
      "Iteration 1990 : Loss 2897.8926\n",
      "Iteration 2000 : Loss 2897.8906\n",
      "Iteration 2010 : Loss 2897.8886\n",
      "Iteration 2020 : Loss 2897.8866\n",
      "Iteration 2030 : Loss 2897.8847\n",
      "Iteration 2040 : Loss 2897.8827\n",
      "Iteration 2050 : Loss 2897.8807\n",
      "Iteration 2060 : Loss 2897.8787\n",
      "Iteration 2070 : Loss 2897.8767\n",
      "Iteration 2080 : Loss 2897.8748\n",
      "Iteration 2090 : Loss 2897.8728\n",
      "Iteration 2100 : Loss 2897.8708\n",
      "Iteration 2110 : Loss 2897.8689\n",
      "Iteration 2120 : Loss 2897.8669\n",
      "Iteration 2130 : Loss 2897.8650\n",
      "Iteration 2140 : Loss 2897.8630\n",
      "Iteration 2150 : Loss 2897.8610\n",
      "Iteration 2160 : Loss 2897.8591\n",
      "Iteration 2170 : Loss 2897.8571\n",
      "Iteration 2180 : Loss 2897.8552\n",
      "Iteration 2190 : Loss 2897.8532\n",
      "Iteration 2200 : Loss 2897.8513\n",
      "Iteration 2210 : Loss 2897.8494\n",
      "Iteration 2220 : Loss 2897.8474\n",
      "Iteration 2230 : Loss 2897.8455\n",
      "Iteration 2240 : Loss 2897.8436\n",
      "Iteration 2250 : Loss 2897.8416\n",
      "Iteration 2260 : Loss 2897.8397\n",
      "Iteration 2270 : Loss 2897.8378\n",
      "Iteration 2280 : Loss 2897.8358\n",
      "Iteration 2290 : Loss 2897.8339\n",
      "Iteration 2300 : Loss 2897.8320\n",
      "Iteration 2310 : Loss 2897.8301\n",
      "Iteration 2320 : Loss 2897.8282\n",
      "Iteration 2330 : Loss 2897.8263\n",
      "Iteration 2340 : Loss 2897.8243\n",
      "Iteration 2350 : Loss 2897.8224\n",
      "Iteration 2360 : Loss 2897.8205\n",
      "Iteration 2370 : Loss 2897.8186\n",
      "Iteration 2380 : Loss 2897.8167\n",
      "Iteration 2390 : Loss 2897.8148\n",
      "Iteration 2400 : Loss 2897.8129\n",
      "Iteration 2410 : Loss 2897.8110\n",
      "Iteration 2420 : Loss 2897.8091\n",
      "Iteration 2430 : Loss 2897.8072\n",
      "Iteration 2440 : Loss 2897.8054\n",
      "Iteration 2450 : Loss 2897.8035\n",
      "Iteration 2460 : Loss 2897.8016\n",
      "Iteration 2470 : Loss 2897.7997\n",
      "Iteration 2480 : Loss 2897.7978\n",
      "Iteration 2490 : Loss 2897.7959\n",
      "Iteration 2500 : Loss 2897.7941\n",
      "Iteration 2510 : Loss 2897.7922\n",
      "Iteration 2520 : Loss 2897.7903\n",
      "Iteration 2530 : Loss 2897.7885\n",
      "Iteration 2540 : Loss 2897.7866\n",
      "Iteration 2550 : Loss 2897.7847\n",
      "Iteration 2560 : Loss 2897.7829\n",
      "Iteration 2570 : Loss 2897.7810\n",
      "Iteration 2580 : Loss 2897.7791\n",
      "Iteration 2590 : Loss 2897.7773\n",
      "Iteration 2600 : Loss 2897.7754\n",
      "Iteration 2610 : Loss 2897.7736\n",
      "Iteration 2620 : Loss 2897.7717\n",
      "Iteration 2630 : Loss 2897.7699\n",
      "Iteration 2640 : Loss 2897.7680\n",
      "Iteration 2650 : Loss 2897.7662\n",
      "Iteration 2660 : Loss 2897.7643\n",
      "Iteration 2670 : Loss 2897.7625\n",
      "Iteration 2680 : Loss 2897.7607\n",
      "Iteration 2690 : Loss 2897.7588\n",
      "Iteration 2700 : Loss 2897.7570\n",
      "Iteration 2710 : Loss 2897.7552\n",
      "Iteration 2720 : Loss 2897.7533\n",
      "Iteration 2730 : Loss 2897.7515\n",
      "Iteration 2740 : Loss 2897.7497\n",
      "Iteration 2750 : Loss 2897.7479\n",
      "Iteration 2760 : Loss 2897.7460\n",
      "Iteration 2770 : Loss 2897.7442\n",
      "Iteration 2780 : Loss 2897.7424\n",
      "Iteration 2790 : Loss 2897.7406\n",
      "Iteration 2800 : Loss 2897.7388\n",
      "Iteration 2810 : Loss 2897.7370\n",
      "Iteration 2820 : Loss 2897.7351\n",
      "Iteration 2830 : Loss 2897.7333\n",
      "Iteration 2840 : Loss 2897.7315\n",
      "Iteration 2850 : Loss 2897.7297\n",
      "Iteration 2860 : Loss 2897.7279\n",
      "Iteration 2870 : Loss 2897.7261\n",
      "Iteration 2880 : Loss 2897.7243\n",
      "Iteration 2890 : Loss 2897.7225\n",
      "Iteration 2900 : Loss 2897.7207\n",
      "Iteration 2910 : Loss 2897.7190\n",
      "Iteration 2920 : Loss 2897.7172\n",
      "Iteration 2930 : Loss 2897.7154\n",
      "Iteration 2940 : Loss 2897.7136\n",
      "Iteration 2950 : Loss 2897.7118\n",
      "Iteration 2960 : Loss 2897.7100\n",
      "Iteration 2970 : Loss 2897.7082\n",
      "Iteration 2980 : Loss 2897.7065\n",
      "Iteration 2990 : Loss 2897.7047\n",
      "Iteration 3000 : Loss 2897.7029\n",
      "Iteration 3010 : Loss 2897.7011\n",
      "Iteration 3020 : Loss 2897.6994\n",
      "Iteration 3030 : Loss 2897.6976\n",
      "Iteration 3040 : Loss 2897.6958\n",
      "Iteration 3050 : Loss 2897.6941\n",
      "Iteration 3060 : Loss 2897.6923\n",
      "Iteration 3070 : Loss 2897.6906\n",
      "Iteration 3080 : Loss 2897.6888\n",
      "Iteration 3090 : Loss 2897.6870\n",
      "Iteration 3100 : Loss 2897.6853\n",
      "Iteration 3110 : Loss 2897.6835\n",
      "Iteration 3120 : Loss 2897.6818\n",
      "Iteration 3130 : Loss 2897.6800\n",
      "Iteration 3140 : Loss 2897.6783\n",
      "Iteration 3150 : Loss 2897.6765\n",
      "Iteration 3160 : Loss 2897.6748\n",
      "Iteration 3170 : Loss 2897.6731\n",
      "Iteration 3180 : Loss 2897.6713\n",
      "Iteration 3190 : Loss 2897.6696\n",
      "Iteration 3200 : Loss 2897.6678\n",
      "Iteration 3210 : Loss 2897.6661\n",
      "Iteration 3220 : Loss 2897.6644\n",
      "Iteration 3230 : Loss 2897.6626\n",
      "Iteration 3240 : Loss 2897.6609\n",
      "Iteration 3250 : Loss 2897.6592\n",
      "Iteration 3260 : Loss 2897.6575\n",
      "Iteration 3270 : Loss 2897.6557\n",
      "Iteration 3280 : Loss 2897.6540\n",
      "Iteration 3290 : Loss 2897.6523\n",
      "Iteration 3300 : Loss 2897.6506\n",
      "Iteration 3310 : Loss 2897.6489\n",
      "Iteration 3320 : Loss 2897.6472\n",
      "Iteration 3330 : Loss 2897.6454\n",
      "Iteration 3340 : Loss 2897.6437\n",
      "Iteration 3350 : Loss 2897.6420\n",
      "Iteration 3360 : Loss 2897.6403\n",
      "Iteration 3370 : Loss 2897.6386\n",
      "Iteration 3380 : Loss 2897.6369\n",
      "Iteration 3390 : Loss 2897.6352\n",
      "Iteration 3400 : Loss 2897.6335\n",
      "Iteration 3410 : Loss 2897.6318\n",
      "Iteration 3420 : Loss 2897.6301\n",
      "Iteration 3430 : Loss 2897.6284\n",
      "Iteration 3440 : Loss 2897.6267\n",
      "Iteration 3450 : Loss 2897.6250\n",
      "Iteration 3460 : Loss 2897.6233\n",
      "Iteration 3470 : Loss 2897.6217\n",
      "Iteration 3480 : Loss 2897.6200\n",
      "Iteration 3490 : Loss 2897.6183\n",
      "Iteration 3500 : Loss 2897.6166\n",
      "Iteration 3510 : Loss 2897.6149\n",
      "Iteration 3520 : Loss 2897.6132\n",
      "Iteration 3530 : Loss 2897.6116\n",
      "Iteration 3540 : Loss 2897.6099\n",
      "Iteration 3550 : Loss 2897.6082\n",
      "Iteration 3560 : Loss 2897.6066\n",
      "Iteration 3570 : Loss 2897.6049\n",
      "Iteration 3580 : Loss 2897.6032\n",
      "Iteration 3590 : Loss 2897.6015\n",
      "Iteration 3600 : Loss 2897.5999\n",
      "Iteration 3610 : Loss 2897.5982\n",
      "Iteration 3620 : Loss 2897.5966\n",
      "Iteration 3630 : Loss 2897.5949\n",
      "Iteration 3640 : Loss 2897.5932\n",
      "Iteration 3650 : Loss 2897.5916\n",
      "Iteration 3660 : Loss 2897.5899\n",
      "Iteration 3670 : Loss 2897.5883\n",
      "Iteration 3680 : Loss 2897.5866\n",
      "Iteration 3690 : Loss 2897.5850\n",
      "Iteration 3700 : Loss 2897.5833\n",
      "Iteration 3710 : Loss 2897.5817\n",
      "Iteration 3720 : Loss 2897.5800\n",
      "Iteration 3730 : Loss 2897.5784\n",
      "Iteration 3740 : Loss 2897.5767\n",
      "Iteration 3750 : Loss 2897.5751\n",
      "Iteration 3760 : Loss 2897.5735\n",
      "Iteration 3770 : Loss 2897.5718\n",
      "Iteration 3780 : Loss 2897.5702\n",
      "Iteration 3790 : Loss 2897.5686\n",
      "Iteration 3800 : Loss 2897.5669\n",
      "Iteration 3810 : Loss 2897.5653\n",
      "Iteration 3820 : Loss 2897.5637\n",
      "Iteration 3830 : Loss 2897.5620\n",
      "Iteration 3840 : Loss 2897.5604\n",
      "Iteration 3850 : Loss 2897.5588\n",
      "Iteration 3860 : Loss 2897.5572\n",
      "Iteration 3870 : Loss 2897.5556\n",
      "Iteration 3880 : Loss 2897.5539\n",
      "Iteration 3890 : Loss 2897.5523\n",
      "Iteration 3900 : Loss 2897.5507\n",
      "Iteration 3910 : Loss 2897.5491\n",
      "Iteration 3920 : Loss 2897.5475\n",
      "Iteration 3930 : Loss 2897.5459\n",
      "Iteration 3940 : Loss 2897.5443\n",
      "Iteration 3950 : Loss 2897.5427\n",
      "Iteration 3960 : Loss 2897.5410\n",
      "Iteration 3970 : Loss 2897.5394\n",
      "Iteration 3980 : Loss 2897.5378\n",
      "Iteration 3990 : Loss 2897.5362\n",
      "Iteration 4000 : Loss 2897.5346\n",
      "Iteration 4010 : Loss 2897.5330\n",
      "Iteration 4020 : Loss 2897.5314\n",
      "Iteration 4030 : Loss 2897.5298\n",
      "Iteration 4040 : Loss 2897.5283\n",
      "Iteration 4050 : Loss 2897.5267\n",
      "Iteration 4060 : Loss 2897.5251\n",
      "Iteration 4070 : Loss 2897.5235\n",
      "Iteration 4080 : Loss 2897.5219\n",
      "Iteration 4090 : Loss 2897.5203\n",
      "Iteration 4100 : Loss 2897.5187\n",
      "Iteration 4110 : Loss 2897.5171\n",
      "Iteration 4120 : Loss 2897.5156\n",
      "Iteration 4130 : Loss 2897.5140\n",
      "Iteration 4140 : Loss 2897.5124\n",
      "Iteration 4150 : Loss 2897.5108\n",
      "Iteration 4160 : Loss 2897.5092\n",
      "Iteration 4170 : Loss 2897.5077\n",
      "Iteration 4180 : Loss 2897.5061\n",
      "Iteration 4190 : Loss 2897.5045\n",
      "Iteration 4200 : Loss 2897.5030\n",
      "Iteration 4210 : Loss 2897.5014\n",
      "Iteration 4220 : Loss 2897.4998\n",
      "Iteration 4230 : Loss 2897.4983\n",
      "Iteration 4240 : Loss 2897.4967\n",
      "Iteration 4250 : Loss 2897.4951\n",
      "Iteration 4260 : Loss 2897.4936\n",
      "Iteration 4270 : Loss 2897.4920\n",
      "Iteration 4280 : Loss 2897.4905\n",
      "Iteration 4290 : Loss 2897.4889\n",
      "Iteration 4300 : Loss 2897.4874\n",
      "Iteration 4310 : Loss 2897.4858\n",
      "Iteration 4320 : Loss 2897.4842\n",
      "Iteration 4330 : Loss 2897.4827\n",
      "Iteration 4340 : Loss 2897.4811\n",
      "Iteration 4350 : Loss 2897.4796\n",
      "Iteration 4360 : Loss 2897.4781\n",
      "Iteration 4370 : Loss 2897.4765\n",
      "Iteration 4380 : Loss 2897.4750\n",
      "Iteration 4390 : Loss 2897.4734\n",
      "Iteration 4400 : Loss 2897.4719\n",
      "Iteration 4410 : Loss 2897.4703\n",
      "Iteration 4420 : Loss 2897.4688\n",
      "Iteration 4430 : Loss 2897.4673\n",
      "Iteration 4440 : Loss 2897.4657\n",
      "Iteration 4450 : Loss 2897.4642\n",
      "Iteration 4460 : Loss 2897.4627\n",
      "Iteration 4470 : Loss 2897.4611\n",
      "Iteration 4480 : Loss 2897.4596\n",
      "Iteration 4490 : Loss 2897.4581\n",
      "Iteration 4500 : Loss 2897.4566\n",
      "Iteration 4510 : Loss 2897.4550\n",
      "Iteration 4520 : Loss 2897.4535\n",
      "Iteration 4530 : Loss 2897.4520\n",
      "Iteration 4540 : Loss 2897.4505\n",
      "Iteration 4550 : Loss 2897.4490\n",
      "Iteration 4560 : Loss 2897.4474\n",
      "Iteration 4570 : Loss 2897.4459\n",
      "Iteration 4580 : Loss 2897.4444\n",
      "Iteration 4590 : Loss 2897.4429\n",
      "Iteration 4600 : Loss 2897.4414\n",
      "Iteration 4610 : Loss 2897.4399\n",
      "Iteration 4620 : Loss 2897.4384\n",
      "Iteration 4630 : Loss 2897.4369\n",
      "Iteration 4640 : Loss 2897.4353\n",
      "Iteration 4650 : Loss 2897.4338\n",
      "Iteration 4660 : Loss 2897.4323\n",
      "Iteration 4670 : Loss 2897.4308\n",
      "Iteration 4680 : Loss 2897.4293\n",
      "Iteration 4690 : Loss 2897.4278\n",
      "Iteration 4700 : Loss 2897.4263\n",
      "Iteration 4710 : Loss 2897.4248\n",
      "Iteration 4720 : Loss 2897.4233\n",
      "Iteration 4730 : Loss 2897.4218\n",
      "Iteration 4740 : Loss 2897.4204\n",
      "Iteration 4750 : Loss 2897.4189\n",
      "Iteration 4760 : Loss 2897.4174\n",
      "Iteration 4770 : Loss 2897.4159\n",
      "Iteration 4780 : Loss 2897.4144\n",
      "Iteration 4790 : Loss 2897.4129\n",
      "Iteration 4800 : Loss 2897.4114\n",
      "Iteration 4810 : Loss 2897.4099\n",
      "Iteration 4820 : Loss 2897.4085\n",
      "Iteration 4830 : Loss 2897.4070\n",
      "Iteration 4840 : Loss 2897.4055\n",
      "Iteration 4850 : Loss 2897.4040\n",
      "Iteration 4860 : Loss 2897.4025\n",
      "Iteration 4870 : Loss 2897.4011\n",
      "Iteration 4880 : Loss 2897.3996\n",
      "Iteration 4890 : Loss 2897.3981\n",
      "Iteration 4900 : Loss 2897.3966\n",
      "Iteration 4910 : Loss 2897.3952\n",
      "Iteration 4920 : Loss 2897.3937\n",
      "Iteration 4930 : Loss 2897.3922\n",
      "Iteration 4940 : Loss 2897.3908\n",
      "Iteration 4950 : Loss 2897.3893\n",
      "Iteration 4960 : Loss 2897.3878\n",
      "Iteration 4970 : Loss 2897.3864\n",
      "Iteration 4980 : Loss 2897.3849\n",
      "Iteration 4990 : Loss 2897.3835\n",
      "Iteration 5000 : Loss 2897.3820\n",
      "Iteration 5010 : Loss 2897.3805\n",
      "Iteration 5020 : Loss 2897.3791\n",
      "Iteration 5030 : Loss 2897.3776\n",
      "Iteration 5040 : Loss 2897.3762\n",
      "Iteration 5050 : Loss 2897.3747\n",
      "Iteration 5060 : Loss 2897.3733\n",
      "Iteration 5070 : Loss 2897.3718\n",
      "Iteration 5080 : Loss 2897.3704\n",
      "Iteration 5090 : Loss 2897.3689\n",
      "Iteration 5100 : Loss 2897.3675\n",
      "Iteration 5110 : Loss 2897.3660\n",
      "Iteration 5120 : Loss 2897.3646\n",
      "Iteration 5130 : Loss 2897.3631\n",
      "Iteration 5140 : Loss 2897.3617\n",
      "Iteration 5150 : Loss 2897.3603\n",
      "Iteration 5160 : Loss 2897.3588\n",
      "Iteration 5170 : Loss 2897.3574\n",
      "Iteration 5180 : Loss 2897.3559\n",
      "Iteration 5190 : Loss 2897.3545\n",
      "Iteration 5200 : Loss 2897.3531\n",
      "Iteration 5210 : Loss 2897.3516\n",
      "Iteration 5220 : Loss 2897.3502\n",
      "Iteration 5230 : Loss 2897.3488\n",
      "Iteration 5240 : Loss 2897.3473\n",
      "Iteration 5250 : Loss 2897.3459\n",
      "Iteration 5260 : Loss 2897.3445\n",
      "Iteration 5270 : Loss 2897.3431\n",
      "Iteration 5280 : Loss 2897.3416\n",
      "Iteration 5290 : Loss 2897.3402\n",
      "Iteration 5300 : Loss 2897.3388\n",
      "Iteration 5310 : Loss 2897.3374\n",
      "Iteration 5320 : Loss 2897.3359\n",
      "Iteration 5330 : Loss 2897.3345\n",
      "Iteration 5340 : Loss 2897.3331\n",
      "Iteration 5350 : Loss 2897.3317\n",
      "Iteration 5360 : Loss 2897.3303\n",
      "Iteration 5370 : Loss 2897.3289\n",
      "Iteration 5380 : Loss 2897.3274\n",
      "Iteration 5390 : Loss 2897.3260\n",
      "Iteration 5400 : Loss 2897.3246\n",
      "Iteration 5410 : Loss 2897.3232\n",
      "Iteration 5420 : Loss 2897.3218\n",
      "Iteration 5430 : Loss 2897.3204\n",
      "Iteration 5440 : Loss 2897.3190\n",
      "Iteration 5450 : Loss 2897.3176\n",
      "Iteration 5460 : Loss 2897.3162\n",
      "Iteration 5470 : Loss 2897.3148\n",
      "Iteration 5480 : Loss 2897.3134\n",
      "Iteration 5490 : Loss 2897.3120\n",
      "Iteration 5500 : Loss 2897.3106\n",
      "Iteration 5510 : Loss 2897.3092\n",
      "Iteration 5520 : Loss 2897.3078\n",
      "Iteration 5530 : Loss 2897.3064\n",
      "Iteration 5540 : Loss 2897.3050\n",
      "Iteration 5550 : Loss 2897.3036\n",
      "Iteration 5560 : Loss 2897.3022\n",
      "Iteration 5570 : Loss 2897.3008\n",
      "Iteration 5580 : Loss 2897.2994\n",
      "Iteration 5590 : Loss 2897.2980\n",
      "Iteration 5600 : Loss 2897.2966\n",
      "Iteration 5610 : Loss 2897.2953\n",
      "Iteration 5620 : Loss 2897.2939\n",
      "Iteration 5630 : Loss 2897.2925\n",
      "Iteration 5640 : Loss 2897.2911\n",
      "Iteration 5650 : Loss 2897.2897\n",
      "Iteration 5660 : Loss 2897.2883\n",
      "Iteration 5670 : Loss 2897.2870\n",
      "Iteration 5680 : Loss 2897.2856\n",
      "Iteration 5690 : Loss 2897.2842\n",
      "Iteration 5700 : Loss 2897.2828\n",
      "Iteration 5710 : Loss 2897.2815\n",
      "Iteration 5720 : Loss 2897.2801\n",
      "Iteration 5730 : Loss 2897.2787\n",
      "Iteration 5740 : Loss 2897.2773\n",
      "Iteration 5750 : Loss 2897.2760\n",
      "Iteration 5760 : Loss 2897.2746\n",
      "Iteration 5770 : Loss 2897.2732\n",
      "Iteration 5780 : Loss 2897.2718\n",
      "Iteration 5790 : Loss 2897.2705\n",
      "Iteration 5800 : Loss 2897.2691\n",
      "Iteration 5810 : Loss 2897.2678\n",
      "Iteration 5820 : Loss 2897.2664\n",
      "Iteration 5830 : Loss 2897.2650\n",
      "Iteration 5840 : Loss 2897.2637\n",
      "Iteration 5850 : Loss 2897.2623\n",
      "Iteration 5860 : Loss 2897.2609\n",
      "Iteration 5870 : Loss 2897.2596\n",
      "Iteration 5880 : Loss 2897.2582\n",
      "Iteration 5890 : Loss 2897.2569\n",
      "Iteration 5900 : Loss 2897.2555\n",
      "Iteration 5910 : Loss 2897.2542\n",
      "Iteration 5920 : Loss 2897.2528\n",
      "Iteration 5930 : Loss 2897.2514\n",
      "Iteration 5940 : Loss 2897.2501\n",
      "Iteration 5950 : Loss 2897.2487\n",
      "Iteration 5960 : Loss 2897.2474\n",
      "Iteration 5970 : Loss 2897.2460\n",
      "Iteration 5980 : Loss 2897.2447\n",
      "Iteration 5990 : Loss 2897.2434\n",
      "Iteration 6000 : Loss 2897.2420\n",
      "Iteration 6010 : Loss 2897.2407\n",
      "Iteration 6020 : Loss 2897.2393\n",
      "Iteration 6030 : Loss 2897.2380\n",
      "Iteration 6040 : Loss 2897.2366\n",
      "Iteration 6050 : Loss 2897.2353\n",
      "Iteration 6060 : Loss 2897.2340\n",
      "Iteration 6070 : Loss 2897.2326\n",
      "Iteration 6080 : Loss 2897.2313\n",
      "Iteration 6090 : Loss 2897.2299\n",
      "Iteration 6100 : Loss 2897.2286\n",
      "Iteration 6110 : Loss 2897.2273\n",
      "Iteration 6120 : Loss 2897.2259\n",
      "Iteration 6130 : Loss 2897.2246\n",
      "Iteration 6140 : Loss 2897.2233\n",
      "Iteration 6150 : Loss 2897.2220\n",
      "Iteration 6160 : Loss 2897.2206\n",
      "Iteration 6170 : Loss 2897.2193\n",
      "Iteration 6180 : Loss 2897.2180\n",
      "Iteration 6190 : Loss 2897.2166\n",
      "Iteration 6200 : Loss 2897.2153\n",
      "Iteration 6210 : Loss 2897.2140\n",
      "Iteration 6220 : Loss 2897.2127\n",
      "Iteration 6230 : Loss 2897.2113\n",
      "Iteration 6240 : Loss 2897.2100\n",
      "Iteration 6250 : Loss 2897.2087\n",
      "Iteration 6260 : Loss 2897.2074\n",
      "Iteration 6270 : Loss 2897.2061\n",
      "Iteration 6280 : Loss 2897.2047\n",
      "Iteration 6290 : Loss 2897.2034\n",
      "Iteration 6300 : Loss 2897.2021\n",
      "Iteration 6310 : Loss 2897.2008\n",
      "Iteration 6320 : Loss 2897.1995\n",
      "Iteration 6330 : Loss 2897.1982\n",
      "Iteration 6340 : Loss 2897.1969\n",
      "Iteration 6350 : Loss 2897.1956\n",
      "Iteration 6360 : Loss 2897.1942\n",
      "Iteration 6370 : Loss 2897.1929\n",
      "Iteration 6380 : Loss 2897.1916\n",
      "Iteration 6390 : Loss 2897.1903\n",
      "Iteration 6400 : Loss 2897.1890\n",
      "Iteration 6410 : Loss 2897.1877\n",
      "Iteration 6420 : Loss 2897.1864\n",
      "Iteration 6430 : Loss 2897.1851\n",
      "Iteration 6440 : Loss 2897.1838\n",
      "Iteration 6450 : Loss 2897.1825\n",
      "Iteration 6460 : Loss 2897.1812\n",
      "Iteration 6470 : Loss 2897.1799\n",
      "Iteration 6480 : Loss 2897.1786\n",
      "Iteration 6490 : Loss 2897.1773\n",
      "Iteration 6500 : Loss 2897.1760\n",
      "Iteration 6510 : Loss 2897.1747\n",
      "Iteration 6520 : Loss 2897.1734\n",
      "Iteration 6530 : Loss 2897.1721\n",
      "Iteration 6540 : Loss 2897.1708\n",
      "Iteration 6550 : Loss 2897.1695\n",
      "Iteration 6560 : Loss 2897.1682\n",
      "Iteration 6570 : Loss 2897.1670\n",
      "Iteration 6580 : Loss 2897.1657\n",
      "Iteration 6590 : Loss 2897.1644\n",
      "Iteration 6600 : Loss 2897.1631\n",
      "Iteration 6610 : Loss 2897.1618\n",
      "Iteration 6620 : Loss 2897.1605\n",
      "Iteration 6630 : Loss 2897.1592\n",
      "Iteration 6640 : Loss 2897.1579\n",
      "Iteration 6650 : Loss 2897.1567\n",
      "Iteration 6660 : Loss 2897.1554\n",
      "Iteration 6670 : Loss 2897.1541\n",
      "Iteration 6680 : Loss 2897.1528\n",
      "Iteration 6690 : Loss 2897.1515\n",
      "Iteration 6700 : Loss 2897.1503\n",
      "Iteration 6710 : Loss 2897.1490\n",
      "Iteration 6720 : Loss 2897.1477\n",
      "Iteration 6730 : Loss 2897.1464\n",
      "Iteration 6740 : Loss 2897.1452\n",
      "Iteration 6750 : Loss 2897.1439\n",
      "Iteration 6760 : Loss 2897.1426\n",
      "Iteration 6770 : Loss 2897.1413\n",
      "Iteration 6780 : Loss 2897.1401\n",
      "Iteration 6790 : Loss 2897.1388\n",
      "Iteration 6800 : Loss 2897.1375\n",
      "Iteration 6810 : Loss 2897.1363\n",
      "Iteration 6820 : Loss 2897.1350\n",
      "Iteration 6830 : Loss 2897.1337\n",
      "Iteration 6840 : Loss 2897.1325\n",
      "Iteration 6850 : Loss 2897.1312\n",
      "Iteration 6860 : Loss 2897.1299\n",
      "Iteration 6870 : Loss 2897.1287\n",
      "Iteration 6880 : Loss 2897.1274\n",
      "Iteration 6890 : Loss 2897.1261\n",
      "Iteration 6900 : Loss 2897.1249\n",
      "Iteration 6910 : Loss 2897.1236\n",
      "Iteration 6920 : Loss 2897.1224\n",
      "Iteration 6930 : Loss 2897.1211\n",
      "Iteration 6940 : Loss 2897.1198\n",
      "Iteration 6950 : Loss 2897.1186\n",
      "Iteration 6960 : Loss 2897.1173\n",
      "Iteration 6970 : Loss 2897.1161\n",
      "Iteration 6980 : Loss 2897.1148\n",
      "Iteration 6990 : Loss 2897.1136\n",
      "Iteration 7000 : Loss 2897.1123\n",
      "Iteration 7010 : Loss 2897.1111\n",
      "Iteration 7020 : Loss 2897.1098\n",
      "Iteration 7030 : Loss 2897.1086\n",
      "Iteration 7040 : Loss 2897.1073\n",
      "Iteration 7050 : Loss 2897.1061\n",
      "Iteration 7060 : Loss 2897.1048\n",
      "Iteration 7070 : Loss 2897.1036\n",
      "Iteration 7080 : Loss 2897.1023\n",
      "Iteration 7090 : Loss 2897.1011\n",
      "Iteration 7100 : Loss 2897.0998\n",
      "Iteration 7110 : Loss 2897.0986\n",
      "Iteration 7120 : Loss 2897.0973\n",
      "Iteration 7130 : Loss 2897.0961\n",
      "Iteration 7140 : Loss 2897.0949\n",
      "Iteration 7150 : Loss 2897.0936\n",
      "Iteration 7160 : Loss 2897.0924\n",
      "Iteration 7170 : Loss 2897.0911\n",
      "Iteration 7180 : Loss 2897.0899\n",
      "Iteration 7190 : Loss 2897.0887\n",
      "Iteration 7200 : Loss 2897.0874\n",
      "Iteration 7210 : Loss 2897.0862\n",
      "Iteration 7220 : Loss 2897.0850\n",
      "Iteration 7230 : Loss 2897.0837\n",
      "Iteration 7240 : Loss 2897.0825\n",
      "Iteration 7250 : Loss 2897.0813\n",
      "Iteration 7260 : Loss 2897.0800\n",
      "Iteration 7270 : Loss 2897.0788\n",
      "Iteration 7280 : Loss 2897.0776\n",
      "Iteration 7290 : Loss 2897.0763\n",
      "Iteration 7300 : Loss 2897.0751\n",
      "Iteration 7310 : Loss 2897.0739\n",
      "Iteration 7320 : Loss 2897.0727\n",
      "Iteration 7330 : Loss 2897.0714\n",
      "Iteration 7340 : Loss 2897.0702\n",
      "Iteration 7350 : Loss 2897.0690\n",
      "Iteration 7360 : Loss 2897.0678\n",
      "Iteration 7370 : Loss 2897.0665\n",
      "Iteration 7380 : Loss 2897.0653\n",
      "Iteration 7390 : Loss 2897.0641\n",
      "Iteration 7400 : Loss 2897.0629\n",
      "Iteration 7410 : Loss 2897.0617\n",
      "Iteration 7420 : Loss 2897.0604\n",
      "Iteration 7430 : Loss 2897.0592\n",
      "Iteration 7440 : Loss 2897.0580\n",
      "Iteration 7450 : Loss 2897.0568\n",
      "Iteration 7460 : Loss 2897.0556\n",
      "Iteration 7470 : Loss 2897.0544\n",
      "Iteration 7480 : Loss 2897.0531\n",
      "Iteration 7490 : Loss 2897.0519\n",
      "Iteration 7500 : Loss 2897.0507\n",
      "Iteration 7510 : Loss 2897.0495\n",
      "Iteration 7520 : Loss 2897.0483\n",
      "Iteration 7530 : Loss 2897.0471\n",
      "Iteration 7540 : Loss 2897.0459\n",
      "Iteration 7550 : Loss 2897.0447\n",
      "Iteration 7560 : Loss 2897.0435\n",
      "Iteration 7570 : Loss 2897.0423\n",
      "Iteration 7580 : Loss 2897.0410\n",
      "Iteration 7590 : Loss 2897.0398\n",
      "Iteration 7600 : Loss 2897.0386\n",
      "Iteration 7610 : Loss 2897.0374\n",
      "Iteration 7620 : Loss 2897.0362\n",
      "Iteration 7630 : Loss 2897.0350\n",
      "Iteration 7640 : Loss 2897.0338\n",
      "Iteration 7650 : Loss 2897.0326\n",
      "Iteration 7660 : Loss 2897.0314\n",
      "Iteration 7670 : Loss 2897.0302\n",
      "Iteration 7680 : Loss 2897.0290\n",
      "Iteration 7690 : Loss 2897.0278\n",
      "Iteration 7700 : Loss 2897.0266\n",
      "Iteration 7710 : Loss 2897.0254\n",
      "Iteration 7720 : Loss 2897.0242\n",
      "Iteration 7730 : Loss 2897.0230\n",
      "Iteration 7740 : Loss 2897.0218\n",
      "Iteration 7750 : Loss 2897.0206\n",
      "Iteration 7760 : Loss 2897.0194\n",
      "Iteration 7770 : Loss 2897.0183\n",
      "Iteration 7780 : Loss 2897.0171\n",
      "Iteration 7790 : Loss 2897.0159\n",
      "Iteration 7800 : Loss 2897.0147\n",
      "Iteration 7810 : Loss 2897.0135\n",
      "Iteration 7820 : Loss 2897.0123\n",
      "Iteration 7830 : Loss 2897.0111\n",
      "Iteration 7840 : Loss 2897.0099\n",
      "Iteration 7850 : Loss 2897.0087\n",
      "Iteration 7860 : Loss 2897.0076\n",
      "Iteration 7870 : Loss 2897.0064\n",
      "Iteration 7880 : Loss 2897.0052\n",
      "Iteration 7890 : Loss 2897.0040\n",
      "Iteration 7900 : Loss 2897.0028\n",
      "Iteration 7910 : Loss 2897.0016\n",
      "Iteration 7920 : Loss 2897.0004\n",
      "Iteration 7930 : Loss 2896.9993\n",
      "Iteration 7940 : Loss 2896.9981\n",
      "Iteration 7950 : Loss 2896.9969\n",
      "Iteration 7960 : Loss 2896.9957\n",
      "Iteration 7970 : Loss 2896.9945\n",
      "Iteration 7980 : Loss 2896.9934\n",
      "Iteration 7990 : Loss 2896.9922\n",
      "Iteration 8000 : Loss 2896.9910\n",
      "Iteration 8010 : Loss 2896.9898\n",
      "Iteration 8020 : Loss 2896.9887\n",
      "Iteration 8030 : Loss 2896.9875\n",
      "Iteration 8040 : Loss 2896.9863\n",
      "Iteration 8050 : Loss 2896.9851\n",
      "Iteration 8060 : Loss 2896.9840\n",
      "Iteration 8070 : Loss 2896.9828\n",
      "Iteration 8080 : Loss 2896.9816\n",
      "Iteration 8090 : Loss 2896.9805\n",
      "Iteration 8100 : Loss 2896.9793\n",
      "Iteration 8110 : Loss 2896.9781\n",
      "Iteration 8120 : Loss 2896.9769\n",
      "Iteration 8130 : Loss 2896.9758\n",
      "Iteration 8140 : Loss 2896.9746\n",
      "Iteration 8150 : Loss 2896.9734\n",
      "Iteration 8160 : Loss 2896.9723\n",
      "Iteration 8170 : Loss 2896.9711\n",
      "Iteration 8180 : Loss 2896.9700\n",
      "Iteration 8190 : Loss 2896.9688\n",
      "Iteration 8200 : Loss 2896.9676\n",
      "Iteration 8210 : Loss 2896.9665\n",
      "Iteration 8220 : Loss 2896.9653\n",
      "Iteration 8230 : Loss 2896.9641\n",
      "Iteration 8240 : Loss 2896.9630\n",
      "Iteration 8250 : Loss 2896.9618\n",
      "Iteration 8260 : Loss 2896.9607\n",
      "Iteration 8270 : Loss 2896.9595\n",
      "Iteration 8280 : Loss 2896.9583\n",
      "Iteration 8290 : Loss 2896.9572\n",
      "Iteration 8300 : Loss 2896.9560\n",
      "Iteration 8310 : Loss 2896.9549\n",
      "Iteration 8320 : Loss 2896.9537\n",
      "Iteration 8330 : Loss 2896.9526\n",
      "Iteration 8340 : Loss 2896.9514\n",
      "Iteration 8350 : Loss 2896.9503\n",
      "Iteration 8360 : Loss 2896.9491\n",
      "Iteration 8370 : Loss 2896.9479\n",
      "Iteration 8380 : Loss 2896.9468\n",
      "Iteration 8390 : Loss 2896.9456\n",
      "Iteration 8400 : Loss 2896.9445\n",
      "Iteration 8410 : Loss 2896.9433\n",
      "Iteration 8420 : Loss 2896.9422\n",
      "Iteration 8430 : Loss 2896.9410\n",
      "Iteration 8440 : Loss 2896.9399\n",
      "Iteration 8450 : Loss 2896.9388\n",
      "Iteration 8460 : Loss 2896.9376\n",
      "Iteration 8470 : Loss 2896.9365\n",
      "Iteration 8480 : Loss 2896.9353\n",
      "Iteration 8490 : Loss 2896.9342\n",
      "Iteration 8500 : Loss 2896.9330\n",
      "Iteration 8510 : Loss 2896.9319\n",
      "Iteration 8520 : Loss 2896.9307\n",
      "Iteration 8530 : Loss 2896.9296\n",
      "Iteration 8540 : Loss 2896.9285\n",
      "Iteration 8550 : Loss 2896.9273\n",
      "Iteration 8560 : Loss 2896.9262\n",
      "Iteration 8570 : Loss 2896.9250\n",
      "Iteration 8580 : Loss 2896.9239\n",
      "Iteration 8590 : Loss 2896.9228\n",
      "Iteration 8600 : Loss 2896.9216\n",
      "Iteration 8610 : Loss 2896.9205\n",
      "Iteration 8620 : Loss 2896.9193\n",
      "Iteration 8630 : Loss 2896.9182\n",
      "Iteration 8640 : Loss 2896.9171\n",
      "Iteration 8650 : Loss 2896.9159\n",
      "Iteration 8660 : Loss 2896.9148\n",
      "Iteration 8670 : Loss 2896.9137\n",
      "Iteration 8680 : Loss 2896.9125\n",
      "Iteration 8690 : Loss 2896.9114\n",
      "Iteration 8700 : Loss 2896.9103\n",
      "Iteration 8710 : Loss 2896.9091\n",
      "Iteration 8720 : Loss 2896.9080\n",
      "Iteration 8730 : Loss 2896.9069\n",
      "Iteration 8740 : Loss 2896.9058\n",
      "Iteration 8750 : Loss 2896.9046\n",
      "Iteration 8760 : Loss 2896.9035\n",
      "Iteration 8770 : Loss 2896.9024\n",
      "Iteration 8780 : Loss 2896.9012\n",
      "Iteration 8790 : Loss 2896.9001\n",
      "Iteration 8800 : Loss 2896.8990\n",
      "Iteration 8810 : Loss 2896.8979\n",
      "Iteration 8820 : Loss 2896.8967\n",
      "Iteration 8830 : Loss 2896.8956\n",
      "Iteration 8840 : Loss 2896.8945\n",
      "Iteration 8850 : Loss 2896.8934\n",
      "Iteration 8860 : Loss 2896.8922\n",
      "Iteration 8870 : Loss 2896.8911\n",
      "Iteration 8880 : Loss 2896.8900\n",
      "Iteration 8890 : Loss 2896.8889\n",
      "Iteration 8900 : Loss 2896.8878\n",
      "Iteration 8910 : Loss 2896.8866\n",
      "Iteration 8920 : Loss 2896.8855\n",
      "Iteration 8930 : Loss 2896.8844\n",
      "Iteration 8940 : Loss 2896.8833\n",
      "Iteration 8950 : Loss 2896.8822\n",
      "Iteration 8960 : Loss 2896.8811\n",
      "Iteration 8970 : Loss 2896.8799\n",
      "Iteration 8980 : Loss 2896.8788\n",
      "Iteration 8990 : Loss 2896.8777\n",
      "Iteration 9000 : Loss 2896.8766\n",
      "Iteration 9010 : Loss 2896.8755\n",
      "Iteration 9020 : Loss 2896.8744\n",
      "Iteration 9030 : Loss 2896.8733\n",
      "Iteration 9040 : Loss 2896.8721\n",
      "Iteration 9050 : Loss 2896.8710\n",
      "Iteration 9060 : Loss 2896.8699\n",
      "Iteration 9070 : Loss 2896.8688\n",
      "Iteration 9080 : Loss 2896.8677\n",
      "Iteration 9090 : Loss 2896.8666\n",
      "Iteration 9100 : Loss 2896.8655\n",
      "Iteration 9110 : Loss 2896.8644\n",
      "Iteration 9120 : Loss 2896.8633\n",
      "Iteration 9130 : Loss 2896.8622\n",
      "Iteration 9140 : Loss 2896.8611\n",
      "Iteration 9150 : Loss 2896.8599\n",
      "Iteration 9160 : Loss 2896.8588\n",
      "Iteration 9170 : Loss 2896.8577\n",
      "Iteration 9180 : Loss 2896.8566\n",
      "Iteration 9190 : Loss 2896.8555\n",
      "Iteration 9200 : Loss 2896.8544\n",
      "Iteration 9210 : Loss 2896.8533\n",
      "Iteration 9220 : Loss 2896.8522\n",
      "Iteration 9230 : Loss 2896.8511\n",
      "Iteration 9240 : Loss 2896.8500\n",
      "Iteration 9250 : Loss 2896.8489\n",
      "Iteration 9260 : Loss 2896.8478\n",
      "Iteration 9270 : Loss 2896.8467\n",
      "Iteration 9280 : Loss 2896.8456\n",
      "Iteration 9290 : Loss 2896.8445\n",
      "Iteration 9300 : Loss 2896.8434\n",
      "Iteration 9310 : Loss 2896.8423\n",
      "Iteration 9320 : Loss 2896.8412\n",
      "Iteration 9330 : Loss 2896.8401\n",
      "Iteration 9340 : Loss 2896.8390\n",
      "Iteration 9350 : Loss 2896.8379\n",
      "Iteration 9360 : Loss 2896.8368\n",
      "Iteration 9370 : Loss 2896.8357\n",
      "Iteration 9380 : Loss 2896.8347\n",
      "Iteration 9390 : Loss 2896.8336\n",
      "Iteration 9400 : Loss 2896.8325\n",
      "Iteration 9410 : Loss 2896.8314\n",
      "Iteration 9420 : Loss 2896.8303\n",
      "Iteration 9430 : Loss 2896.8292\n",
      "Iteration 9440 : Loss 2896.8281\n",
      "Iteration 9450 : Loss 2896.8270\n",
      "Iteration 9460 : Loss 2896.8259\n",
      "Iteration 9470 : Loss 2896.8248\n",
      "Iteration 9480 : Loss 2896.8237\n",
      "Iteration 9490 : Loss 2896.8227\n",
      "Iteration 9500 : Loss 2896.8216\n",
      "Iteration 9510 : Loss 2896.8205\n",
      "Iteration 9520 : Loss 2896.8194\n",
      "Iteration 9530 : Loss 2896.8183\n",
      "Iteration 9540 : Loss 2896.8172\n",
      "Iteration 9550 : Loss 2896.8161\n",
      "Iteration 9560 : Loss 2896.8151\n",
      "Iteration 9570 : Loss 2896.8140\n",
      "Iteration 9580 : Loss 2896.8129\n",
      "Iteration 9590 : Loss 2896.8118\n",
      "Iteration 9600 : Loss 2896.8107\n",
      "Iteration 9610 : Loss 2896.8096\n",
      "Iteration 9620 : Loss 2896.8086\n",
      "Iteration 9630 : Loss 2896.8075\n",
      "Iteration 9640 : Loss 2896.8064\n",
      "Iteration 9650 : Loss 2896.8053\n",
      "Iteration 9660 : Loss 2896.8042\n",
      "Iteration 9670 : Loss 2896.8032\n",
      "Iteration 9680 : Loss 2896.8021\n",
      "Iteration 9690 : Loss 2896.8010\n",
      "Iteration 9700 : Loss 2896.7999\n",
      "Iteration 9710 : Loss 2896.7988\n",
      "Iteration 9720 : Loss 2896.7978\n",
      "Iteration 9730 : Loss 2896.7967\n",
      "Iteration 9740 : Loss 2896.7956\n",
      "Iteration 9750 : Loss 2896.7945\n",
      "Iteration 9760 : Loss 2896.7935\n",
      "Iteration 9770 : Loss 2896.7924\n",
      "Iteration 9780 : Loss 2896.7913\n",
      "Iteration 9790 : Loss 2896.7902\n",
      "Iteration 9800 : Loss 2896.7892\n",
      "Iteration 9810 : Loss 2896.7881\n",
      "Iteration 9820 : Loss 2896.7870\n",
      "Iteration 9830 : Loss 2896.7860\n",
      "Iteration 9840 : Loss 2896.7849\n",
      "Iteration 9850 : Loss 2896.7838\n",
      "Iteration 9860 : Loss 2896.7828\n",
      "Iteration 9870 : Loss 2896.7817\n",
      "Iteration 9880 : Loss 2896.7806\n",
      "Iteration 9890 : Loss 2896.7795\n",
      "Iteration 9900 : Loss 2896.7785\n",
      "Iteration 9910 : Loss 2896.7774\n",
      "Iteration 9920 : Loss 2896.7763\n",
      "Iteration 9930 : Loss 2896.7753\n",
      "Iteration 9940 : Loss 2896.7742\n",
      "Iteration 9950 : Loss 2896.7731\n",
      "Iteration 9960 : Loss 2896.7721\n",
      "Iteration 9970 : Loss 2896.7710\n",
      "Iteration 9980 : Loss 2896.7700\n",
      "Iteration 9990 : Loss 2896.7689\n",
      "Iteration 10000 : Loss 2896.7678\n",
      "Iteration 10010 : Loss 2896.7668\n",
      "Iteration 10020 : Loss 2896.7657\n",
      "Iteration 10030 : Loss 2896.7646\n",
      "Iteration 10040 : Loss 2896.7636\n",
      "Iteration 10050 : Loss 2896.7625\n",
      "Iteration 10060 : Loss 2896.7615\n",
      "Iteration 10070 : Loss 2896.7604\n",
      "Iteration 10080 : Loss 2896.7593\n",
      "Iteration 10090 : Loss 2896.7583\n",
      "Iteration 10100 : Loss 2896.7572\n",
      "Iteration 10110 : Loss 2896.7562\n",
      "Iteration 10120 : Loss 2896.7551\n",
      "Iteration 10130 : Loss 2896.7540\n",
      "Iteration 10140 : Loss 2896.7530\n",
      "Iteration 10150 : Loss 2896.7519\n",
      "Iteration 10160 : Loss 2896.7509\n",
      "Iteration 10170 : Loss 2896.7498\n",
      "Iteration 10180 : Loss 2896.7488\n",
      "Iteration 10190 : Loss 2896.7477\n",
      "Iteration 10200 : Loss 2896.7467\n",
      "Iteration 10210 : Loss 2896.7456\n",
      "Iteration 10220 : Loss 2896.7446\n",
      "Iteration 10230 : Loss 2896.7435\n",
      "Iteration 10240 : Loss 2896.7424\n",
      "Iteration 10250 : Loss 2896.7414\n",
      "Iteration 10260 : Loss 2896.7403\n",
      "Iteration 10270 : Loss 2896.7393\n",
      "Iteration 10280 : Loss 2896.7382\n",
      "Iteration 10290 : Loss 2896.7372\n",
      "Iteration 10300 : Loss 2896.7361\n",
      "Iteration 10310 : Loss 2896.7351\n",
      "Iteration 10320 : Loss 2896.7340\n",
      "Iteration 10330 : Loss 2896.7330\n",
      "Iteration 10340 : Loss 2896.7320\n",
      "Iteration 10350 : Loss 2896.7309\n",
      "Iteration 10360 : Loss 2896.7299\n",
      "Iteration 10370 : Loss 2896.7288\n",
      "Iteration 10380 : Loss 2896.7278\n",
      "Iteration 10390 : Loss 2896.7267\n",
      "Iteration 10400 : Loss 2896.7257\n",
      "Iteration 10410 : Loss 2896.7246\n",
      "Iteration 10420 : Loss 2896.7236\n",
      "Iteration 10430 : Loss 2896.7225\n",
      "Iteration 10440 : Loss 2896.7215\n",
      "Iteration 10450 : Loss 2896.7205\n",
      "Iteration 10460 : Loss 2896.7194\n",
      "Iteration 10470 : Loss 2896.7184\n",
      "Iteration 10480 : Loss 2896.7173\n",
      "Iteration 10490 : Loss 2896.7163\n",
      "Iteration 10500 : Loss 2896.7152\n",
      "Iteration 10510 : Loss 2896.7142\n",
      "Iteration 10520 : Loss 2896.7132\n",
      "Iteration 10530 : Loss 2896.7121\n",
      "Iteration 10540 : Loss 2896.7111\n",
      "Iteration 10550 : Loss 2896.7100\n",
      "Iteration 10560 : Loss 2896.7090\n",
      "Iteration 10570 : Loss 2896.7080\n",
      "Iteration 10580 : Loss 2896.7069\n",
      "Iteration 10590 : Loss 2896.7059\n",
      "Iteration 10600 : Loss 2896.7049\n",
      "Iteration 10610 : Loss 2896.7038\n",
      "Iteration 10620 : Loss 2896.7028\n",
      "Iteration 10630 : Loss 2896.7018\n",
      "Iteration 10640 : Loss 2896.7007\n",
      "Iteration 10650 : Loss 2896.6997\n",
      "Iteration 10660 : Loss 2896.6986\n",
      "Iteration 10670 : Loss 2896.6976\n",
      "Iteration 10680 : Loss 2896.6966\n",
      "Iteration 10690 : Loss 2896.6955\n",
      "Iteration 10700 : Loss 2896.6945\n",
      "Iteration 10710 : Loss 2896.6935\n",
      "Iteration 10720 : Loss 2896.6925\n",
      "Iteration 10730 : Loss 2896.6914\n",
      "Iteration 10740 : Loss 2896.6904\n",
      "Iteration 10750 : Loss 2896.6894\n",
      "Iteration 10760 : Loss 2896.6883\n",
      "Iteration 10770 : Loss 2896.6873\n",
      "Iteration 10780 : Loss 2896.6863\n",
      "Iteration 10790 : Loss 2896.6852\n",
      "Iteration 10800 : Loss 2896.6842\n",
      "Iteration 10810 : Loss 2896.6832\n",
      "Iteration 10820 : Loss 2896.6822\n",
      "Iteration 10830 : Loss 2896.6811\n",
      "Iteration 10840 : Loss 2896.6801\n",
      "Iteration 10850 : Loss 2896.6791\n",
      "Iteration 10860 : Loss 2896.6781\n",
      "Iteration 10870 : Loss 2896.6770\n",
      "Iteration 10880 : Loss 2896.6760\n",
      "Iteration 10890 : Loss 2896.6750\n",
      "Iteration 10900 : Loss 2896.6740\n",
      "Iteration 10910 : Loss 2896.6729\n",
      "Iteration 10920 : Loss 2896.6719\n",
      "Iteration 10930 : Loss 2896.6709\n",
      "Iteration 10940 : Loss 2896.6699\n",
      "Iteration 10950 : Loss 2896.6688\n",
      "Iteration 10960 : Loss 2896.6678\n",
      "Iteration 10970 : Loss 2896.6668\n",
      "Iteration 10980 : Loss 2896.6658\n",
      "Iteration 10990 : Loss 2896.6647\n",
      "Iteration 11000 : Loss 2896.6637\n",
      "Iteration 11010 : Loss 2896.6627\n",
      "Iteration 11020 : Loss 2896.6617\n",
      "Iteration 11030 : Loss 2896.6607\n",
      "Iteration 11040 : Loss 2896.6596\n",
      "Iteration 11050 : Loss 2896.6586\n",
      "Iteration 11060 : Loss 2896.6576\n",
      "Iteration 11070 : Loss 2896.6566\n",
      "Iteration 11080 : Loss 2896.6556\n",
      "Iteration 11090 : Loss 2896.6546\n",
      "Iteration 11100 : Loss 2896.6535\n",
      "Iteration 11110 : Loss 2896.6525\n",
      "Iteration 11120 : Loss 2896.6515\n",
      "Iteration 11130 : Loss 2896.6505\n",
      "Iteration 11140 : Loss 2896.6495\n",
      "Iteration 11150 : Loss 2896.6485\n",
      "Iteration 11160 : Loss 2896.6474\n",
      "Iteration 11170 : Loss 2896.6464\n",
      "Iteration 11180 : Loss 2896.6454\n",
      "Iteration 11190 : Loss 2896.6444\n",
      "Iteration 11200 : Loss 2896.6434\n",
      "Iteration 11210 : Loss 2896.6424\n",
      "Iteration 11220 : Loss 2896.6414\n",
      "Iteration 11230 : Loss 2896.6404\n",
      "Iteration 11240 : Loss 2896.6393\n",
      "Iteration 11250 : Loss 2896.6383\n",
      "Iteration 11260 : Loss 2896.6373\n",
      "Iteration 11270 : Loss 2896.6363\n",
      "Iteration 11280 : Loss 2896.6353\n",
      "Iteration 11290 : Loss 2896.6343\n",
      "Iteration 11300 : Loss 2896.6333\n",
      "Iteration 11310 : Loss 2896.6323\n",
      "Iteration 11320 : Loss 2896.6313\n",
      "Iteration 11330 : Loss 2896.6303\n",
      "Iteration 11340 : Loss 2896.6292\n",
      "Iteration 11350 : Loss 2896.6282\n",
      "Iteration 11360 : Loss 2896.6272\n",
      "Iteration 11370 : Loss 2896.6262\n",
      "Iteration 11380 : Loss 2896.6252\n",
      "Iteration 11390 : Loss 2896.6242\n",
      "Iteration 11400 : Loss 2896.6232\n",
      "Iteration 11410 : Loss 2896.6222\n",
      "Iteration 11420 : Loss 2896.6212\n",
      "Iteration 11430 : Loss 2896.6202\n",
      "Iteration 11440 : Loss 2896.6192\n",
      "Iteration 11450 : Loss 2896.6182\n",
      "Iteration 11460 : Loss 2896.6172\n",
      "Iteration 11470 : Loss 2896.6162\n",
      "Iteration 11480 : Loss 2896.6152\n",
      "Iteration 11490 : Loss 2896.6142\n",
      "Iteration 11500 : Loss 2896.6132\n",
      "Iteration 11510 : Loss 2896.6122\n",
      "Iteration 11520 : Loss 2896.6112\n",
      "Iteration 11530 : Loss 2896.6102\n",
      "Iteration 11540 : Loss 2896.6092\n",
      "Iteration 11550 : Loss 2896.6082\n",
      "Iteration 11560 : Loss 2896.6072\n",
      "Iteration 11570 : Loss 2896.6062\n",
      "Iteration 11580 : Loss 2896.6052\n",
      "Iteration 11590 : Loss 2896.6042\n",
      "Iteration 11600 : Loss 2896.6032\n",
      "Iteration 11610 : Loss 2896.6022\n",
      "Iteration 11620 : Loss 2896.6012\n",
      "Iteration 11630 : Loss 2896.6002\n",
      "Iteration 11640 : Loss 2896.5992\n",
      "Iteration 11650 : Loss 2896.5982\n",
      "Iteration 11660 : Loss 2896.5972\n",
      "Iteration 11670 : Loss 2896.5962\n",
      "Iteration 11680 : Loss 2896.5952\n",
      "Iteration 11690 : Loss 2896.5942\n",
      "Iteration 11700 : Loss 2896.5932\n",
      "Iteration 11710 : Loss 2896.5922\n",
      "Iteration 11720 : Loss 2896.5912\n",
      "Iteration 11730 : Loss 2896.5902\n",
      "Iteration 11740 : Loss 2896.5892\n",
      "Iteration 11750 : Loss 2896.5882\n",
      "Iteration 11760 : Loss 2896.5872\n",
      "Iteration 11770 : Loss 2896.5862\n",
      "Iteration 11780 : Loss 2896.5853\n",
      "Iteration 11790 : Loss 2896.5843\n",
      "Iteration 11800 : Loss 2896.5833\n",
      "Iteration 11810 : Loss 2896.5823\n",
      "Iteration 11820 : Loss 2896.5813\n",
      "Iteration 11830 : Loss 2896.5803\n",
      "Iteration 11840 : Loss 2896.5793\n",
      "Iteration 11850 : Loss 2896.5783\n",
      "Iteration 11860 : Loss 2896.5773\n",
      "Iteration 11870 : Loss 2896.5763\n",
      "Iteration 11880 : Loss 2896.5753\n",
      "Iteration 11890 : Loss 2896.5744\n",
      "Iteration 11900 : Loss 2896.5734\n",
      "Iteration 11910 : Loss 2896.5724\n",
      "Iteration 11920 : Loss 2896.5714\n",
      "Iteration 11930 : Loss 2896.5704\n",
      "Iteration 11940 : Loss 2896.5694\n",
      "Iteration 11950 : Loss 2896.5684\n",
      "Iteration 11960 : Loss 2896.5674\n",
      "Iteration 11970 : Loss 2896.5665\n",
      "Iteration 11980 : Loss 2896.5655\n",
      "Iteration 11990 : Loss 2896.5645\n",
      "Iteration 12000 : Loss 2896.5635\n",
      "Iteration 12010 : Loss 2896.5625\n",
      "Iteration 12020 : Loss 2896.5615\n",
      "Iteration 12030 : Loss 2896.5605\n",
      "Iteration 12040 : Loss 2896.5596\n",
      "Iteration 12050 : Loss 2896.5586\n",
      "Iteration 12060 : Loss 2896.5576\n",
      "Iteration 12070 : Loss 2896.5566\n",
      "Iteration 12080 : Loss 2896.5556\n",
      "Iteration 12090 : Loss 2896.5546\n",
      "Iteration 12100 : Loss 2896.5537\n",
      "Iteration 12110 : Loss 2896.5527\n",
      "Iteration 12120 : Loss 2896.5517\n",
      "Iteration 12130 : Loss 2896.5507\n",
      "Iteration 12140 : Loss 2896.5497\n",
      "Iteration 12150 : Loss 2896.5488\n",
      "Iteration 12160 : Loss 2896.5478\n",
      "Iteration 12170 : Loss 2896.5468\n",
      "Iteration 12180 : Loss 2896.5458\n",
      "Iteration 12190 : Loss 2896.5448\n",
      "Iteration 12200 : Loss 2896.5439\n",
      "Iteration 12210 : Loss 2896.5429\n",
      "Iteration 12220 : Loss 2896.5419\n",
      "Iteration 12230 : Loss 2896.5409\n",
      "Iteration 12240 : Loss 2896.5399\n",
      "Iteration 12250 : Loss 2896.5390\n",
      "Iteration 12260 : Loss 2896.5380\n",
      "Iteration 12270 : Loss 2896.5370\n",
      "Iteration 12280 : Loss 2896.5360\n",
      "Iteration 12290 : Loss 2896.5351\n",
      "Iteration 12300 : Loss 2896.5341\n",
      "Iteration 12310 : Loss 2896.5331\n",
      "Iteration 12320 : Loss 2896.5321\n",
      "Iteration 12330 : Loss 2896.5312\n",
      "Iteration 12340 : Loss 2896.5302\n",
      "Iteration 12350 : Loss 2896.5292\n",
      "Iteration 12360 : Loss 2896.5282\n",
      "Iteration 12370 : Loss 2896.5273\n",
      "Iteration 12380 : Loss 2896.5263\n",
      "Iteration 12390 : Loss 2896.5253\n",
      "Iteration 12400 : Loss 2896.5243\n",
      "Iteration 12410 : Loss 2896.5234\n",
      "Iteration 12420 : Loss 2896.5224\n",
      "Iteration 12430 : Loss 2896.5214\n",
      "Iteration 12440 : Loss 2896.5204\n",
      "Iteration 12450 : Loss 2896.5195\n",
      "Iteration 12460 : Loss 2896.5185\n",
      "Iteration 12470 : Loss 2896.5175\n",
      "Iteration 12480 : Loss 2896.5166\n",
      "Iteration 12490 : Loss 2896.5156\n",
      "Iteration 12500 : Loss 2896.5146\n",
      "Iteration 12510 : Loss 2896.5137\n",
      "Iteration 12520 : Loss 2896.5127\n",
      "Iteration 12530 : Loss 2896.5117\n",
      "Iteration 12540 : Loss 2896.5107\n",
      "Iteration 12550 : Loss 2896.5098\n",
      "Iteration 12560 : Loss 2896.5088\n",
      "Iteration 12570 : Loss 2896.5078\n",
      "Iteration 12580 : Loss 2896.5069\n",
      "Iteration 12590 : Loss 2896.5059\n",
      "Iteration 12600 : Loss 2896.5049\n",
      "Iteration 12610 : Loss 2896.5040\n",
      "Iteration 12620 : Loss 2896.5030\n",
      "Iteration 12630 : Loss 2896.5020\n",
      "Iteration 12640 : Loss 2896.5011\n",
      "Iteration 12650 : Loss 2896.5001\n",
      "Iteration 12660 : Loss 2896.4991\n",
      "Iteration 12670 : Loss 2896.4982\n",
      "Iteration 12680 : Loss 2896.4972\n",
      "Iteration 12690 : Loss 2896.4963\n",
      "Iteration 12700 : Loss 2896.4953\n",
      "Iteration 12710 : Loss 2896.4943\n",
      "Iteration 12720 : Loss 2896.4934\n",
      "Iteration 12730 : Loss 2896.4924\n",
      "Iteration 12740 : Loss 2896.4914\n",
      "Iteration 12750 : Loss 2896.4905\n",
      "Iteration 12760 : Loss 2896.4895\n",
      "Iteration 12770 : Loss 2896.4885\n",
      "Iteration 12780 : Loss 2896.4876\n",
      "Iteration 12790 : Loss 2896.4866\n",
      "Iteration 12800 : Loss 2896.4857\n",
      "Iteration 12810 : Loss 2896.4847\n",
      "Iteration 12820 : Loss 2896.4837\n",
      "Iteration 12830 : Loss 2896.4828\n",
      "Iteration 12840 : Loss 2896.4818\n",
      "Iteration 12850 : Loss 2896.4809\n",
      "Iteration 12860 : Loss 2896.4799\n",
      "Iteration 12870 : Loss 2896.4789\n",
      "Iteration 12880 : Loss 2896.4780\n",
      "Iteration 12890 : Loss 2896.4770\n",
      "Iteration 12900 : Loss 2896.4761\n",
      "Iteration 12910 : Loss 2896.4751\n",
      "Iteration 12920 : Loss 2896.4741\n",
      "Iteration 12930 : Loss 2896.4732\n",
      "Iteration 12940 : Loss 2896.4722\n",
      "Iteration 12950 : Loss 2896.4713\n",
      "Iteration 12960 : Loss 2896.4703\n",
      "Iteration 12970 : Loss 2896.4694\n",
      "Iteration 12980 : Loss 2896.4684\n",
      "Iteration 12990 : Loss 2896.4674\n",
      "Iteration 13000 : Loss 2896.4665\n",
      "Iteration 13010 : Loss 2896.4655\n",
      "Iteration 13020 : Loss 2896.4646\n",
      "Iteration 13030 : Loss 2896.4636\n",
      "Iteration 13040 : Loss 2896.4627\n",
      "Iteration 13050 : Loss 2896.4617\n",
      "Iteration 13060 : Loss 2896.4608\n",
      "Iteration 13070 : Loss 2896.4598\n",
      "Iteration 13080 : Loss 2896.4588\n",
      "Iteration 13090 : Loss 2896.4579\n",
      "Iteration 13100 : Loss 2896.4569\n",
      "Iteration 13110 : Loss 2896.4560\n",
      "Iteration 13120 : Loss 2896.4550\n",
      "Iteration 13130 : Loss 2896.4541\n",
      "Iteration 13140 : Loss 2896.4531\n",
      "Iteration 13150 : Loss 2896.4522\n",
      "Iteration 13160 : Loss 2896.4512\n",
      "Iteration 13170 : Loss 2896.4503\n",
      "Iteration 13180 : Loss 2896.4493\n",
      "Iteration 13190 : Loss 2896.4484\n",
      "Iteration 13200 : Loss 2896.4474\n",
      "Iteration 13210 : Loss 2896.4465\n",
      "Iteration 13220 : Loss 2896.4455\n",
      "Iteration 13230 : Loss 2896.4446\n",
      "Iteration 13240 : Loss 2896.4436\n",
      "Iteration 13250 : Loss 2896.4427\n",
      "Iteration 13260 : Loss 2896.4417\n",
      "Iteration 13270 : Loss 2896.4408\n",
      "Iteration 13280 : Loss 2896.4398\n",
      "Iteration 13290 : Loss 2896.4389\n",
      "Iteration 13300 : Loss 2896.4379\n",
      "Iteration 13310 : Loss 2896.4370\n",
      "Iteration 13320 : Loss 2896.4360\n",
      "Iteration 13330 : Loss 2896.4351\n",
      "Iteration 13340 : Loss 2896.4341\n",
      "Iteration 13350 : Loss 2896.4332\n",
      "Iteration 13360 : Loss 2896.4322\n",
      "Iteration 13370 : Loss 2896.4313\n",
      "Iteration 13380 : Loss 2896.4304\n",
      "Iteration 13390 : Loss 2896.4294\n",
      "Iteration 13400 : Loss 2896.4285\n",
      "Iteration 13410 : Loss 2896.4275\n",
      "Iteration 13420 : Loss 2896.4266\n",
      "Iteration 13430 : Loss 2896.4256\n",
      "Iteration 13440 : Loss 2896.4247\n",
      "Iteration 13450 : Loss 2896.4237\n",
      "Iteration 13460 : Loss 2896.4228\n",
      "Iteration 13470 : Loss 2896.4219\n",
      "Iteration 13480 : Loss 2896.4209\n",
      "Iteration 13490 : Loss 2896.4200\n",
      "Iteration 13500 : Loss 2896.4190\n",
      "Iteration 13510 : Loss 2896.4181\n",
      "Iteration 13520 : Loss 2896.4171\n",
      "Iteration 13530 : Loss 2896.4162\n",
      "Iteration 13540 : Loss 2896.4153\n",
      "Iteration 13550 : Loss 2896.4143\n",
      "Iteration 13560 : Loss 2896.4134\n",
      "Iteration 13570 : Loss 2896.4124\n",
      "Iteration 13580 : Loss 2896.4115\n",
      "Iteration 13590 : Loss 2896.4105\n",
      "Iteration 13600 : Loss 2896.4096\n",
      "Iteration 13610 : Loss 2896.4087\n",
      "Iteration 13620 : Loss 2896.4077\n",
      "Iteration 13630 : Loss 2896.4068\n",
      "Iteration 13640 : Loss 2896.4058\n",
      "Iteration 13650 : Loss 2896.4049\n",
      "Iteration 13660 : Loss 2896.4040\n",
      "Iteration 13670 : Loss 2896.4030\n",
      "Iteration 13680 : Loss 2896.4021\n",
      "Iteration 13690 : Loss 2896.4011\n",
      "Iteration 13700 : Loss 2896.4002\n",
      "Iteration 13710 : Loss 2896.3993\n",
      "Iteration 13720 : Loss 2896.3983\n",
      "Iteration 13730 : Loss 2896.3974\n",
      "Iteration 13740 : Loss 2896.3965\n",
      "Iteration 13750 : Loss 2896.3955\n",
      "Iteration 13760 : Loss 2896.3946\n",
      "Iteration 13770 : Loss 2896.3936\n",
      "Iteration 13780 : Loss 2896.3927\n",
      "Iteration 13790 : Loss 2896.3918\n",
      "Iteration 13800 : Loss 2896.3908\n",
      "Iteration 13810 : Loss 2896.3899\n",
      "Iteration 13820 : Loss 2896.3890\n",
      "Iteration 13830 : Loss 2896.3880\n",
      "Iteration 13840 : Loss 2896.3871\n",
      "Iteration 13850 : Loss 2896.3862\n",
      "Iteration 13860 : Loss 2896.3852\n",
      "Iteration 13870 : Loss 2896.3843\n",
      "Iteration 13880 : Loss 2896.3834\n",
      "Iteration 13890 : Loss 2896.3824\n",
      "Iteration 13900 : Loss 2896.3815\n",
      "Iteration 13910 : Loss 2896.3806\n",
      "Iteration 13920 : Loss 2896.3796\n",
      "Iteration 13930 : Loss 2896.3787\n",
      "Iteration 13940 : Loss 2896.3778\n",
      "Iteration 13950 : Loss 2896.3768\n",
      "Iteration 13960 : Loss 2896.3759\n",
      "Iteration 13970 : Loss 2896.3750\n",
      "Iteration 13980 : Loss 2896.3740\n",
      "Iteration 13990 : Loss 2896.3731\n",
      "Iteration 14000 : Loss 2896.3722\n",
      "Iteration 14010 : Loss 2896.3712\n",
      "Iteration 14020 : Loss 2896.3703\n",
      "Iteration 14030 : Loss 2896.3694\n",
      "Iteration 14040 : Loss 2896.3684\n",
      "Iteration 14050 : Loss 2896.3675\n",
      "Iteration 14060 : Loss 2896.3666\n",
      "Iteration 14070 : Loss 2896.3657\n",
      "Iteration 14080 : Loss 2896.3647\n",
      "Iteration 14090 : Loss 2896.3638\n",
      "Iteration 14100 : Loss 2896.3629\n",
      "Iteration 14110 : Loss 2896.3619\n",
      "Iteration 14120 : Loss 2896.3610\n",
      "Iteration 14130 : Loss 2896.3601\n",
      "Iteration 14140 : Loss 2896.3592\n",
      "Iteration 14150 : Loss 2896.3582\n",
      "Iteration 14160 : Loss 2896.3573\n",
      "Iteration 14170 : Loss 2896.3564\n",
      "Iteration 14180 : Loss 2896.3554\n",
      "Iteration 14190 : Loss 2896.3545\n",
      "Iteration 14200 : Loss 2896.3536\n",
      "Iteration 14210 : Loss 2896.3527\n",
      "Iteration 14220 : Loss 2896.3517\n",
      "Iteration 14230 : Loss 2896.3508\n",
      "Iteration 14240 : Loss 2896.3499\n",
      "Iteration 14250 : Loss 2896.3490\n",
      "Iteration 14260 : Loss 2896.3480\n",
      "Iteration 14270 : Loss 2896.3471\n",
      "Iteration 14280 : Loss 2896.3462\n",
      "Iteration 14290 : Loss 2896.3453\n",
      "Iteration 14300 : Loss 2896.3443\n",
      "Iteration 14310 : Loss 2896.3434\n",
      "Iteration 14320 : Loss 2896.3425\n",
      "Iteration 14330 : Loss 2896.3416\n",
      "Iteration 14340 : Loss 2896.3406\n",
      "Iteration 14350 : Loss 2896.3397\n",
      "Iteration 14360 : Loss 2896.3388\n",
      "Iteration 14370 : Loss 2896.3379\n",
      "Iteration 14380 : Loss 2896.3369\n",
      "Iteration 14390 : Loss 2896.3360\n",
      "Iteration 14400 : Loss 2896.3351\n",
      "Iteration 14410 : Loss 2896.3342\n",
      "Iteration 14420 : Loss 2896.3333\n",
      "Iteration 14430 : Loss 2896.3323\n",
      "Iteration 14440 : Loss 2896.3314\n",
      "Iteration 14450 : Loss 2896.3305\n",
      "Iteration 14460 : Loss 2896.3296\n",
      "Iteration 14470 : Loss 2896.3286\n",
      "Iteration 14480 : Loss 2896.3277\n",
      "Iteration 14490 : Loss 2896.3268\n",
      "Iteration 14500 : Loss 2896.3259\n",
      "Iteration 14510 : Loss 2896.3250\n",
      "Iteration 14520 : Loss 2896.3240\n",
      "Iteration 14530 : Loss 2896.3231\n",
      "Iteration 14540 : Loss 2896.3222\n",
      "Iteration 14550 : Loss 2896.3213\n",
      "Iteration 14560 : Loss 2896.3204\n",
      "Iteration 14570 : Loss 2896.3194\n",
      "Iteration 14580 : Loss 2896.3185\n",
      "Iteration 14590 : Loss 2896.3176\n",
      "Iteration 14600 : Loss 2896.3167\n",
      "Iteration 14610 : Loss 2896.3158\n",
      "Iteration 14620 : Loss 2896.3149\n",
      "Iteration 14630 : Loss 2896.3139\n",
      "Iteration 14640 : Loss 2896.3130\n",
      "Iteration 14650 : Loss 2896.3121\n",
      "Iteration 14660 : Loss 2896.3112\n",
      "Iteration 14670 : Loss 2896.3103\n",
      "Iteration 14680 : Loss 2896.3093\n",
      "Iteration 14690 : Loss 2896.3084\n",
      "Iteration 14700 : Loss 2896.3075\n",
      "Iteration 14710 : Loss 2896.3066\n",
      "Iteration 14720 : Loss 2896.3057\n",
      "Iteration 14730 : Loss 2896.3048\n",
      "Iteration 14740 : Loss 2896.3038\n",
      "Iteration 14750 : Loss 2896.3029\n",
      "Iteration 14760 : Loss 2896.3020\n",
      "Iteration 14770 : Loss 2896.3011\n",
      "Iteration 14780 : Loss 2896.3002\n",
      "Iteration 14790 : Loss 2896.2993\n",
      "Iteration 14800 : Loss 2896.2984\n",
      "Iteration 14810 : Loss 2896.2974\n",
      "Iteration 14820 : Loss 2896.2965\n",
      "Iteration 14830 : Loss 2896.2956\n",
      "Iteration 14840 : Loss 2896.2947\n",
      "Iteration 14850 : Loss 2896.2938\n",
      "Iteration 14860 : Loss 2896.2929\n",
      "Iteration 14870 : Loss 2896.2920\n",
      "Iteration 14880 : Loss 2896.2911\n",
      "Iteration 14890 : Loss 2896.2901\n",
      "Iteration 14900 : Loss 2896.2892\n",
      "Iteration 14910 : Loss 2896.2883\n",
      "Iteration 14920 : Loss 2896.2874\n",
      "Iteration 14930 : Loss 2896.2865\n",
      "Iteration 14940 : Loss 2896.2856\n",
      "Iteration 14950 : Loss 2896.2847\n",
      "Iteration 14960 : Loss 2896.2838\n",
      "Iteration 14970 : Loss 2896.2828\n",
      "Iteration 14980 : Loss 2896.2819\n",
      "Iteration 14990 : Loss 2896.2810\n",
      "Iteration 15000 : Loss 2896.2801\n",
      "Iteration 15010 : Loss 2896.2792\n",
      "Iteration 15020 : Loss 2896.2783\n",
      "Iteration 15030 : Loss 2896.2774\n",
      "Iteration 15040 : Loss 2896.2765\n",
      "Iteration 15050 : Loss 2896.2756\n",
      "Iteration 15060 : Loss 2896.2747\n",
      "Iteration 15070 : Loss 2896.2737\n",
      "Iteration 15080 : Loss 2896.2728\n",
      "Iteration 15090 : Loss 2896.2719\n",
      "Iteration 15100 : Loss 2896.2710\n",
      "Iteration 15110 : Loss 2896.2701\n",
      "Iteration 15120 : Loss 2896.2692\n",
      "Iteration 15130 : Loss 2896.2683\n",
      "Iteration 15140 : Loss 2896.2674\n",
      "Iteration 15150 : Loss 2896.2665\n",
      "Iteration 15160 : Loss 2896.2656\n",
      "Iteration 15170 : Loss 2896.2647\n",
      "Iteration 15180 : Loss 2896.2638\n",
      "Iteration 15190 : Loss 2896.2628\n",
      "Iteration 15200 : Loss 2896.2619\n",
      "Iteration 15210 : Loss 2896.2610\n",
      "Iteration 15220 : Loss 2896.2601\n",
      "Iteration 15230 : Loss 2896.2592\n",
      "Iteration 15240 : Loss 2896.2583\n",
      "Iteration 15250 : Loss 2896.2574\n",
      "Iteration 15260 : Loss 2896.2565\n",
      "Iteration 15270 : Loss 2896.2556\n",
      "Iteration 15280 : Loss 2896.2547\n",
      "Iteration 15290 : Loss 2896.2538\n",
      "Iteration 15300 : Loss 2896.2529\n",
      "Iteration 15310 : Loss 2896.2520\n",
      "Iteration 15320 : Loss 2896.2511\n",
      "Iteration 15330 : Loss 2896.2502\n",
      "Iteration 15340 : Loss 2896.2493\n",
      "Iteration 15350 : Loss 2896.2484\n",
      "Iteration 15360 : Loss 2896.2475\n",
      "Iteration 15370 : Loss 2896.2465\n",
      "Iteration 15380 : Loss 2896.2456\n",
      "Iteration 15390 : Loss 2896.2447\n",
      "Iteration 15400 : Loss 2896.2438\n",
      "Iteration 15410 : Loss 2896.2429\n",
      "Iteration 15420 : Loss 2896.2420\n",
      "Iteration 15430 : Loss 2896.2411\n",
      "Iteration 15440 : Loss 2896.2402\n",
      "Iteration 15450 : Loss 2896.2393\n",
      "Iteration 15460 : Loss 2896.2384\n",
      "Iteration 15470 : Loss 2896.2375\n",
      "Iteration 15480 : Loss 2896.2366\n",
      "Iteration 15490 : Loss 2896.2357\n",
      "Iteration 15500 : Loss 2896.2348\n",
      "Iteration 15510 : Loss 2896.2339\n",
      "Iteration 15520 : Loss 2896.2330\n",
      "Iteration 15530 : Loss 2896.2321\n",
      "Iteration 15540 : Loss 2896.2312\n",
      "Iteration 15550 : Loss 2896.2303\n",
      "Iteration 15560 : Loss 2896.2294\n",
      "Iteration 15570 : Loss 2896.2285\n",
      "Iteration 15580 : Loss 2896.2276\n",
      "Iteration 15590 : Loss 2896.2267\n",
      "Iteration 15600 : Loss 2896.2258\n",
      "Iteration 15610 : Loss 2896.2249\n",
      "Iteration 15620 : Loss 2896.2240\n",
      "Iteration 15630 : Loss 2896.2231\n",
      "Iteration 15640 : Loss 2896.2222\n",
      "Iteration 15650 : Loss 2896.2213\n",
      "Iteration 15660 : Loss 2896.2204\n",
      "Iteration 15670 : Loss 2896.2195\n",
      "Iteration 15680 : Loss 2896.2186\n",
      "Iteration 15690 : Loss 2896.2177\n",
      "Iteration 15700 : Loss 2896.2168\n",
      "Iteration 15710 : Loss 2896.2159\n",
      "Iteration 15720 : Loss 2896.2150\n",
      "Iteration 15730 : Loss 2896.2141\n",
      "Iteration 15740 : Loss 2896.2132\n",
      "Iteration 15750 : Loss 2896.2123\n",
      "Iteration 15760 : Loss 2896.2114\n",
      "Iteration 15770 : Loss 2896.2105\n",
      "Iteration 15780 : Loss 2896.2096\n",
      "Iteration 15790 : Loss 2896.2087\n",
      "Iteration 15800 : Loss 2896.2079\n",
      "Iteration 15810 : Loss 2896.2070\n",
      "Iteration 15820 : Loss 2896.2061\n",
      "Iteration 15830 : Loss 2896.2052\n",
      "Iteration 15840 : Loss 2896.2043\n",
      "Iteration 15850 : Loss 2896.2034\n",
      "Iteration 15860 : Loss 2896.2025\n",
      "Iteration 15870 : Loss 2896.2016\n",
      "Iteration 15880 : Loss 2896.2007\n",
      "Iteration 15890 : Loss 2896.1998\n",
      "Iteration 15900 : Loss 2896.1989\n",
      "Iteration 15910 : Loss 2896.1980\n",
      "Iteration 15920 : Loss 2896.1971\n",
      "Iteration 15930 : Loss 2896.1962\n",
      "Iteration 15940 : Loss 2896.1953\n",
      "Iteration 15950 : Loss 2896.1944\n",
      "Iteration 15960 : Loss 2896.1935\n",
      "Iteration 15970 : Loss 2896.1926\n",
      "Iteration 15980 : Loss 2896.1917\n",
      "Iteration 15990 : Loss 2896.1909\n",
      "Iteration 16000 : Loss 2896.1900\n",
      "Iteration 16010 : Loss 2896.1891\n",
      "Iteration 16020 : Loss 2896.1882\n",
      "Iteration 16030 : Loss 2896.1873\n",
      "Iteration 16040 : Loss 2896.1864\n",
      "Iteration 16050 : Loss 2896.1855\n",
      "Iteration 16060 : Loss 2896.1846\n",
      "Iteration 16070 : Loss 2896.1837\n",
      "Iteration 16080 : Loss 2896.1828\n",
      "Iteration 16090 : Loss 2896.1819\n",
      "Iteration 16100 : Loss 2896.1810\n",
      "Iteration 16110 : Loss 2896.1801\n",
      "Iteration 16120 : Loss 2896.1793\n",
      "Iteration 16130 : Loss 2896.1784\n",
      "Iteration 16140 : Loss 2896.1775\n",
      "Iteration 16150 : Loss 2896.1766\n",
      "Iteration 16160 : Loss 2896.1757\n",
      "Iteration 16170 : Loss 2896.1748\n",
      "Iteration 16180 : Loss 2896.1739\n",
      "Iteration 16190 : Loss 2896.1730\n",
      "Iteration 16200 : Loss 2896.1721\n",
      "Iteration 16210 : Loss 2896.1712\n",
      "Iteration 16220 : Loss 2896.1704\n",
      "Iteration 16230 : Loss 2896.1695\n",
      "Iteration 16240 : Loss 2896.1686\n",
      "Iteration 16250 : Loss 2896.1677\n",
      "Iteration 16260 : Loss 2896.1668\n",
      "Iteration 16270 : Loss 2896.1659\n",
      "Iteration 16280 : Loss 2896.1650\n",
      "Iteration 16290 : Loss 2896.1641\n",
      "Iteration 16300 : Loss 2896.1632\n",
      "Iteration 16310 : Loss 2896.1624\n",
      "Iteration 16320 : Loss 2896.1615\n",
      "Iteration 16330 : Loss 2896.1606\n",
      "Iteration 16340 : Loss 2896.1597\n",
      "Iteration 16350 : Loss 2896.1588\n",
      "Iteration 16360 : Loss 2896.1579\n",
      "Iteration 16370 : Loss 2896.1570\n",
      "Iteration 16380 : Loss 2896.1561\n",
      "Iteration 16390 : Loss 2896.1553\n",
      "Iteration 16400 : Loss 2896.1544\n",
      "Iteration 16410 : Loss 2896.1535\n",
      "Iteration 16420 : Loss 2896.1526\n",
      "Iteration 16430 : Loss 2896.1517\n",
      "Iteration 16440 : Loss 2896.1508\n",
      "Iteration 16450 : Loss 2896.1499\n",
      "Iteration 16460 : Loss 2896.1490\n",
      "Iteration 16470 : Loss 2896.1482\n",
      "Iteration 16480 : Loss 2896.1473\n",
      "Iteration 16490 : Loss 2896.1464\n",
      "Iteration 16500 : Loss 2896.1455\n",
      "Iteration 16510 : Loss 2896.1446\n",
      "Iteration 16520 : Loss 2896.1437\n",
      "Iteration 16530 : Loss 2896.1428\n",
      "Iteration 16540 : Loss 2896.1420\n",
      "Iteration 16550 : Loss 2896.1411\n",
      "Iteration 16560 : Loss 2896.1402\n",
      "Iteration 16570 : Loss 2896.1393\n",
      "Iteration 16580 : Loss 2896.1384\n",
      "Iteration 16590 : Loss 2896.1375\n",
      "Iteration 16600 : Loss 2896.1367\n",
      "Iteration 16610 : Loss 2896.1358\n",
      "Iteration 16620 : Loss 2896.1349\n",
      "Iteration 16630 : Loss 2896.1340\n",
      "Iteration 16640 : Loss 2896.1331\n",
      "Iteration 16650 : Loss 2896.1322\n",
      "Iteration 16660 : Loss 2896.1314\n",
      "Iteration 16670 : Loss 2896.1305\n",
      "Iteration 16680 : Loss 2896.1296\n",
      "Iteration 16690 : Loss 2896.1287\n",
      "Iteration 16700 : Loss 2896.1278\n",
      "Iteration 16710 : Loss 2896.1269\n",
      "Iteration 16720 : Loss 2896.1261\n",
      "Iteration 16730 : Loss 2896.1252\n",
      "Iteration 16740 : Loss 2896.1243\n",
      "Iteration 16750 : Loss 2896.1234\n",
      "Iteration 16760 : Loss 2896.1225\n",
      "Iteration 16770 : Loss 2896.1217\n",
      "Iteration 16780 : Loss 2896.1208\n",
      "Iteration 16790 : Loss 2896.1199\n",
      "Iteration 16800 : Loss 2896.1190\n",
      "Iteration 16810 : Loss 2896.1181\n",
      "Iteration 16820 : Loss 2896.1173\n",
      "Iteration 16830 : Loss 2896.1164\n",
      "Iteration 16840 : Loss 2896.1155\n",
      "Iteration 16850 : Loss 2896.1146\n",
      "Iteration 16860 : Loss 2896.1137\n",
      "Iteration 16870 : Loss 2896.1129\n",
      "Iteration 16880 : Loss 2896.1120\n",
      "Iteration 16890 : Loss 2896.1111\n",
      "Iteration 16900 : Loss 2896.1102\n",
      "Iteration 16910 : Loss 2896.1093\n",
      "Iteration 16920 : Loss 2896.1085\n",
      "Iteration 16930 : Loss 2896.1076\n",
      "Iteration 16940 : Loss 2896.1067\n",
      "Iteration 16950 : Loss 2896.1058\n",
      "Iteration 16960 : Loss 2896.1049\n",
      "Iteration 16970 : Loss 2896.1041\n",
      "Iteration 16980 : Loss 2896.1032\n",
      "Iteration 16990 : Loss 2896.1023\n",
      "Iteration 17000 : Loss 2896.1014\n",
      "Iteration 17010 : Loss 2896.1005\n",
      "Iteration 17020 : Loss 2896.0997\n",
      "Iteration 17030 : Loss 2896.0988\n",
      "Iteration 17040 : Loss 2896.0979\n",
      "Iteration 17050 : Loss 2896.0970\n",
      "Iteration 17060 : Loss 2896.0962\n",
      "Iteration 17070 : Loss 2896.0953\n",
      "Iteration 17080 : Loss 2896.0944\n",
      "Iteration 17090 : Loss 2896.0935\n",
      "Iteration 17100 : Loss 2896.0926\n",
      "Iteration 17110 : Loss 2896.0918\n",
      "Iteration 17120 : Loss 2896.0909\n",
      "Iteration 17130 : Loss 2896.0900\n",
      "Iteration 17140 : Loss 2896.0891\n",
      "Iteration 17150 : Loss 2896.0883\n",
      "Iteration 17160 : Loss 2896.0874\n",
      "Iteration 17170 : Loss 2896.0865\n",
      "Iteration 17180 : Loss 2896.0856\n",
      "Iteration 17190 : Loss 2896.0848\n",
      "Iteration 17200 : Loss 2896.0839\n",
      "Iteration 17210 : Loss 2896.0830\n",
      "Iteration 17220 : Loss 2896.0821\n",
      "Iteration 17230 : Loss 2896.0813\n",
      "Iteration 17240 : Loss 2896.0804\n",
      "Iteration 17250 : Loss 2896.0795\n",
      "Iteration 17260 : Loss 2896.0786\n",
      "Iteration 17270 : Loss 2896.0778\n",
      "Iteration 17280 : Loss 2896.0769\n",
      "Iteration 17290 : Loss 2896.0760\n",
      "Iteration 17300 : Loss 2896.0751\n",
      "Iteration 17310 : Loss 2896.0743\n",
      "Iteration 17320 : Loss 2896.0734\n",
      "Iteration 17330 : Loss 2896.0725\n",
      "Iteration 17340 : Loss 2896.0716\n",
      "Iteration 17350 : Loss 2896.0708\n",
      "Iteration 17360 : Loss 2896.0699\n",
      "Iteration 17370 : Loss 2896.0690\n",
      "Iteration 17380 : Loss 2896.0682\n",
      "Iteration 17390 : Loss 2896.0673\n",
      "Iteration 17400 : Loss 2896.0664\n",
      "Iteration 17410 : Loss 2896.0655\n",
      "Iteration 17420 : Loss 2896.0647\n",
      "Iteration 17430 : Loss 2896.0638\n",
      "Iteration 17440 : Loss 2896.0629\n",
      "Iteration 17450 : Loss 2896.0620\n",
      "Iteration 17460 : Loss 2896.0612\n",
      "Iteration 17470 : Loss 2896.0603\n",
      "Iteration 17480 : Loss 2896.0594\n",
      "Iteration 17490 : Loss 2896.0586\n",
      "Iteration 17500 : Loss 2896.0577\n",
      "Iteration 17510 : Loss 2896.0568\n",
      "Iteration 17520 : Loss 2896.0559\n",
      "Iteration 17530 : Loss 2896.0551\n",
      "Iteration 17540 : Loss 2896.0542\n",
      "Iteration 17550 : Loss 2896.0533\n",
      "Iteration 17560 : Loss 2896.0525\n",
      "Iteration 17570 : Loss 2896.0516\n",
      "Iteration 17580 : Loss 2896.0507\n",
      "Iteration 17590 : Loss 2896.0498\n",
      "Iteration 17600 : Loss 2896.0490\n",
      "Iteration 17610 : Loss 2896.0481\n",
      "Iteration 17620 : Loss 2896.0472\n",
      "Iteration 17630 : Loss 2896.0464\n",
      "Iteration 17640 : Loss 2896.0455\n",
      "Iteration 17650 : Loss 2896.0446\n",
      "Iteration 17660 : Loss 2896.0438\n",
      "Iteration 17670 : Loss 2896.0429\n",
      "Iteration 17680 : Loss 2896.0420\n",
      "Iteration 17690 : Loss 2896.0412\n",
      "Iteration 17700 : Loss 2896.0403\n",
      "Iteration 17710 : Loss 2896.0394\n",
      "Iteration 17720 : Loss 2896.0385\n",
      "Iteration 17730 : Loss 2896.0377\n",
      "Iteration 17740 : Loss 2896.0368\n",
      "Iteration 17750 : Loss 2896.0359\n",
      "Iteration 17760 : Loss 2896.0351\n",
      "Iteration 17770 : Loss 2896.0342\n",
      "Iteration 17780 : Loss 2896.0333\n",
      "Iteration 17790 : Loss 2896.0325\n",
      "Iteration 17800 : Loss 2896.0316\n",
      "Iteration 17810 : Loss 2896.0307\n",
      "Iteration 17820 : Loss 2896.0299\n",
      "Iteration 17830 : Loss 2896.0290\n",
      "Iteration 17840 : Loss 2896.0281\n",
      "Iteration 17850 : Loss 2896.0273\n",
      "Iteration 17860 : Loss 2896.0264\n",
      "Iteration 17870 : Loss 2896.0255\n",
      "Iteration 17880 : Loss 2896.0247\n",
      "Iteration 17890 : Loss 2896.0238\n",
      "Iteration 17900 : Loss 2896.0229\n",
      "Iteration 17910 : Loss 2896.0221\n",
      "Iteration 17920 : Loss 2896.0212\n",
      "Iteration 17930 : Loss 2896.0203\n",
      "Iteration 17940 : Loss 2896.0195\n",
      "Iteration 17950 : Loss 2896.0186\n",
      "Iteration 17960 : Loss 2896.0177\n",
      "Iteration 17970 : Loss 2896.0169\n",
      "Iteration 17980 : Loss 2896.0160\n",
      "Iteration 17990 : Loss 2896.0151\n",
      "Iteration 18000 : Loss 2896.0143\n",
      "Iteration 18010 : Loss 2896.0134\n",
      "Iteration 18020 : Loss 2896.0125\n",
      "Iteration 18030 : Loss 2896.0117\n",
      "Iteration 18040 : Loss 2896.0108\n",
      "Iteration 18050 : Loss 2896.0099\n",
      "Iteration 18060 : Loss 2896.0091\n",
      "Iteration 18070 : Loss 2896.0082\n",
      "Iteration 18080 : Loss 2896.0073\n",
      "Iteration 18090 : Loss 2896.0065\n",
      "Iteration 18100 : Loss 2896.0056\n",
      "Iteration 18110 : Loss 2896.0048\n",
      "Iteration 18120 : Loss 2896.0039\n",
      "Iteration 18130 : Loss 2896.0030\n",
      "Iteration 18140 : Loss 2896.0022\n",
      "Iteration 18150 : Loss 2896.0013\n",
      "Iteration 18160 : Loss 2896.0004\n",
      "Iteration 18170 : Loss 2895.9996\n",
      "Iteration 18180 : Loss 2895.9987\n",
      "Iteration 18190 : Loss 2895.9978\n",
      "Iteration 18200 : Loss 2895.9970\n",
      "Iteration 18210 : Loss 2895.9961\n",
      "Iteration 18220 : Loss 2895.9953\n",
      "Iteration 18230 : Loss 2895.9944\n",
      "Iteration 18240 : Loss 2895.9935\n",
      "Iteration 18250 : Loss 2895.9927\n",
      "Iteration 18260 : Loss 2895.9918\n",
      "Iteration 18270 : Loss 2895.9909\n",
      "Iteration 18280 : Loss 2895.9901\n",
      "Iteration 18290 : Loss 2895.9892\n",
      "Iteration 18300 : Loss 2895.9884\n",
      "Iteration 18310 : Loss 2895.9875\n",
      "Iteration 18320 : Loss 2895.9866\n",
      "Iteration 18330 : Loss 2895.9858\n",
      "Iteration 18340 : Loss 2895.9849\n",
      "Iteration 18350 : Loss 2895.9841\n",
      "Iteration 18360 : Loss 2895.9832\n",
      "Iteration 18370 : Loss 2895.9823\n",
      "Iteration 18380 : Loss 2895.9815\n",
      "Iteration 18390 : Loss 2895.9806\n",
      "Iteration 18400 : Loss 2895.9797\n",
      "Iteration 18410 : Loss 2895.9789\n",
      "Iteration 18420 : Loss 2895.9780\n",
      "Iteration 18430 : Loss 2895.9772\n",
      "Iteration 18440 : Loss 2895.9763\n",
      "Iteration 18450 : Loss 2895.9754\n",
      "Iteration 18460 : Loss 2895.9746\n",
      "Iteration 18470 : Loss 2895.9737\n",
      "Iteration 18480 : Loss 2895.9729\n",
      "Iteration 18490 : Loss 2895.9720\n",
      "Iteration 18500 : Loss 2895.9711\n",
      "Iteration 18510 : Loss 2895.9703\n",
      "Iteration 18520 : Loss 2895.9694\n",
      "Iteration 18530 : Loss 2895.9686\n",
      "Iteration 18540 : Loss 2895.9677\n",
      "Iteration 18550 : Loss 2895.9668\n",
      "Iteration 18560 : Loss 2895.9660\n",
      "Iteration 18570 : Loss 2895.9651\n",
      "Iteration 18580 : Loss 2895.9643\n",
      "Iteration 18590 : Loss 2895.9634\n",
      "Iteration 18600 : Loss 2895.9626\n",
      "Iteration 18610 : Loss 2895.9617\n",
      "Iteration 18620 : Loss 2895.9608\n",
      "Iteration 18630 : Loss 2895.9600\n",
      "Iteration 18640 : Loss 2895.9591\n",
      "Iteration 18650 : Loss 2895.9583\n",
      "Iteration 18660 : Loss 2895.9574\n",
      "Iteration 18670 : Loss 2895.9565\n",
      "Iteration 18680 : Loss 2895.9557\n",
      "Iteration 18690 : Loss 2895.9548\n",
      "Iteration 18700 : Loss 2895.9540\n",
      "Iteration 18710 : Loss 2895.9531\n",
      "Iteration 18720 : Loss 2895.9523\n",
      "Iteration 18730 : Loss 2895.9514\n",
      "Iteration 18740 : Loss 2895.9505\n",
      "Iteration 18750 : Loss 2895.9497\n",
      "Iteration 18760 : Loss 2895.9488\n",
      "Iteration 18770 : Loss 2895.9480\n",
      "Iteration 18780 : Loss 2895.9471\n",
      "Iteration 18790 : Loss 2895.9463\n",
      "Iteration 18800 : Loss 2895.9454\n",
      "Iteration 18810 : Loss 2895.9445\n",
      "Iteration 18820 : Loss 2895.9437\n",
      "Iteration 18830 : Loss 2895.9428\n",
      "Iteration 18840 : Loss 2895.9420\n",
      "Iteration 18850 : Loss 2895.9411\n",
      "Iteration 18860 : Loss 2895.9403\n",
      "Iteration 18870 : Loss 2895.9394\n",
      "Iteration 18880 : Loss 2895.9386\n",
      "Iteration 18890 : Loss 2895.9377\n",
      "Iteration 18900 : Loss 2895.9368\n",
      "Iteration 18910 : Loss 2895.9360\n",
      "Iteration 18920 : Loss 2895.9351\n",
      "Iteration 18930 : Loss 2895.9343\n",
      "Iteration 18940 : Loss 2895.9334\n",
      "Iteration 18950 : Loss 2895.9326\n",
      "Iteration 18960 : Loss 2895.9317\n",
      "Iteration 18970 : Loss 2895.9309\n",
      "Iteration 18980 : Loss 2895.9300\n",
      "Iteration 18990 : Loss 2895.9291\n",
      "Iteration 19000 : Loss 2895.9283\n",
      "Iteration 19010 : Loss 2895.9274\n",
      "Iteration 19020 : Loss 2895.9266\n",
      "Iteration 19030 : Loss 2895.9257\n",
      "Iteration 19040 : Loss 2895.9249\n",
      "Iteration 19050 : Loss 2895.9240\n",
      "Iteration 19060 : Loss 2895.9232\n",
      "Iteration 19070 : Loss 2895.9223\n",
      "Iteration 19080 : Loss 2895.9215\n",
      "Iteration 19090 : Loss 2895.9206\n",
      "Iteration 19100 : Loss 2895.9198\n",
      "Iteration 19110 : Loss 2895.9189\n",
      "Iteration 19120 : Loss 2895.9180\n",
      "Iteration 19130 : Loss 2895.9172\n",
      "Iteration 19140 : Loss 2895.9163\n",
      "Iteration 19150 : Loss 2895.9155\n",
      "Iteration 19160 : Loss 2895.9146\n",
      "Iteration 19170 : Loss 2895.9138\n",
      "Iteration 19180 : Loss 2895.9129\n",
      "Iteration 19190 : Loss 2895.9121\n",
      "Iteration 19200 : Loss 2895.9112\n",
      "Iteration 19210 : Loss 2895.9104\n",
      "Iteration 19220 : Loss 2895.9095\n",
      "Iteration 19230 : Loss 2895.9087\n",
      "Iteration 19240 : Loss 2895.9078\n",
      "Iteration 19250 : Loss 2895.9070\n",
      "Iteration 19260 : Loss 2895.9061\n",
      "Iteration 19270 : Loss 2895.9053\n",
      "Iteration 19280 : Loss 2895.9044\n",
      "Iteration 19290 : Loss 2895.9036\n",
      "Iteration 19300 : Loss 2895.9027\n",
      "Iteration 19310 : Loss 2895.9019\n",
      "Iteration 19320 : Loss 2895.9010\n",
      "Iteration 19330 : Loss 2895.9002\n",
      "Iteration 19340 : Loss 2895.8993\n",
      "Iteration 19350 : Loss 2895.8985\n",
      "Iteration 19360 : Loss 2895.8976\n",
      "Iteration 19370 : Loss 2895.8968\n",
      "Iteration 19380 : Loss 2895.8959\n",
      "Iteration 19390 : Loss 2895.8951\n",
      "Iteration 19400 : Loss 2895.8942\n",
      "Iteration 19410 : Loss 2895.8933\n",
      "Iteration 19420 : Loss 2895.8925\n",
      "Iteration 19430 : Loss 2895.8916\n",
      "Iteration 19440 : Loss 2895.8908\n",
      "Iteration 19450 : Loss 2895.8899\n",
      "Iteration 19460 : Loss 2895.8891\n",
      "Iteration 19470 : Loss 2895.8882\n",
      "Iteration 19480 : Loss 2895.8874\n",
      "Iteration 19490 : Loss 2895.8866\n",
      "Iteration 19500 : Loss 2895.8857\n",
      "Iteration 19510 : Loss 2895.8849\n",
      "Iteration 19520 : Loss 2895.8840\n",
      "Iteration 19530 : Loss 2895.8832\n",
      "Iteration 19540 : Loss 2895.8823\n",
      "Iteration 19550 : Loss 2895.8815\n",
      "Iteration 19560 : Loss 2895.8806\n",
      "Iteration 19570 : Loss 2895.8798\n",
      "Iteration 19580 : Loss 2895.8789\n",
      "Iteration 19590 : Loss 2895.8781\n",
      "Iteration 19600 : Loss 2895.8772\n",
      "Iteration 19610 : Loss 2895.8764\n",
      "Iteration 19620 : Loss 2895.8755\n",
      "Iteration 19630 : Loss 2895.8747\n",
      "Iteration 19640 : Loss 2895.8738\n",
      "Iteration 19650 : Loss 2895.8730\n",
      "Iteration 19660 : Loss 2895.8721\n",
      "Iteration 19670 : Loss 2895.8713\n",
      "Iteration 19680 : Loss 2895.8704\n",
      "Iteration 19690 : Loss 2895.8696\n",
      "Iteration 19700 : Loss 2895.8687\n",
      "Iteration 19710 : Loss 2895.8679\n",
      "Iteration 19720 : Loss 2895.8670\n",
      "Iteration 19730 : Loss 2895.8662\n",
      "Iteration 19740 : Loss 2895.8653\n",
      "Iteration 19750 : Loss 2895.8645\n",
      "Iteration 19760 : Loss 2895.8636\n",
      "Iteration 19770 : Loss 2895.8628\n",
      "Iteration 19780 : Loss 2895.8620\n",
      "Iteration 19790 : Loss 2895.8611\n",
      "Iteration 19800 : Loss 2895.8603\n",
      "Iteration 19810 : Loss 2895.8594\n",
      "Iteration 19820 : Loss 2895.8586\n",
      "Iteration 19830 : Loss 2895.8577\n",
      "Iteration 19840 : Loss 2895.8569\n",
      "Iteration 19850 : Loss 2895.8560\n",
      "Iteration 19860 : Loss 2895.8552\n",
      "Iteration 19870 : Loss 2895.8543\n",
      "Iteration 19880 : Loss 2895.8535\n",
      "Iteration 19890 : Loss 2895.8526\n",
      "Iteration 19900 : Loss 2895.8518\n",
      "Iteration 19910 : Loss 2895.8510\n",
      "Iteration 19920 : Loss 2895.8501\n",
      "Iteration 19930 : Loss 2895.8493\n",
      "Iteration 19940 : Loss 2895.8484\n",
      "Iteration 19950 : Loss 2895.8476\n",
      "Iteration 19960 : Loss 2895.8467\n",
      "Iteration 19970 : Loss 2895.8459\n",
      "Iteration 19980 : Loss 2895.8450\n",
      "Iteration 19990 : Loss 2895.8442\n",
      "Iteration 20000 : Loss 2895.8433\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "# 반복횟수가 높아질수록 손실함수의 값이 낮아지지만 시스템자원을 적절하게 활용하기 위해 학습률과 반복횟수의 적절한 제한을 줌.\n",
    "for i in range(1, 20001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNNING_RATE * dW\n",
    "    b -= LEARNNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt는 matplotlib.pyplot에 단축어 그래프를 사용하기 위해 선언\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  39.97077718, -235.82706304,  549.09387641,  339.65086538,\n",
       "         -96.16203644, -127.78992617, -216.67767471,  148.04117921,\n",
       "         409.29268155,   74.21579372]),\n",
       " 151.30461061349337)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) test 데이터에 대한 성능 확인하기\n",
    "test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879.6168385462097"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse #2879.6168385462097\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtLklEQVR4nO2dfZhV1Xnofy/jjA5JdFRQ+SwmpXoJWpDBegs3j9EqiZ80icTcpJoPH7yNNTHpRbG1xrRNQXmq1qRJY0mr9mk+yBcQNbVWTFu8NTIjimjiFTS5MKCAEU3CyMfw3j/OHjjnzN4ze539cdY++/09zzxzzjp77/POmr3Xu9b7tURVMQzDMMrJqGYLYBiGYTQPUwKGYRglxpSAYRhGiTElYBiGUWJMCRiGYZQYUwKGYRglZkQlICJHicgTIvK0iDwrIp8P2u8RkZdE5KngZ0bQLiJyl4hsEpENInJGxn+DYRiG0SBHxDhmL3COqv5KRNqBtSLyw+CzRar6nbrj3wtMDX5+B/hK8NswDMPwjBFXAlrhV8Hb9uBnuAyzS4H7gvMeB7pEZFxyUQ3DMIy0ibMSQETagF7gN4G/VdUfi8gfAl8QkZuBR4DFqroXmABsqTp9a9C2Per6Y8aM0SlTpjT2FxiGYZSU3t7eXao6Nsk1YikBVR0AZohIF/B9EZkO3Ai8DHQAdwM3AH8e94tFZCGwEGDy5Mn09PS4SW4YhlFyROTnSa/hFB2kqruBR4H3qOr2wOSzF/hH4MzgsD5gUtVpE4O2+mvdrardqto9dmwiRWYYhmE0SJzooLHBCgAR6QTOA346aOcXEQHmAxuDU1YDVwRRQmcBr6tqpCnIMAzDaB5xzEHjgHsDv8AoYIWq3i8ia0RkLCDAU8D/Co5/ELgA2ATsAT6WutSGYRhGKoyoBFR1AzAzpP2ciOMVuCa5aIZhGEbWWMawYRhGiYkVHWQYRrlYub6PZQ89z7bd/Yzv6mTRvFOYP3NCs8UyMsCUgGEYNaxc38eN33uG/v0DAPTt7ufG7z0DYIqgBTFzkGEYNSx76PlDCmCQ/v0DLHvo+SZJZGSJKQHDMGrYtrvfqd0oNqYEDMOoYXxXp1O7UWxMCRiGUcOieafQ2d5W09bZ3saieac0SSIjS8wxbBhGDYPOX4sOKgemBAzDGML8mRNs0C8JZg4yDMMoMaYEDMMwSowpAcMwjBJjSsAwDKPEmBIwDMMoMRYdZHiNFTIzjGwxJWB4ixUyM4zsMXOQ4S1WyMwwsseUgOEtVsjMMLLHlIDhLVbIzDCyx5SA4S1WyMwwssccw4a3pFXIzCKMDCMaUwKG1yQtZGYRRoYxPGYOMloaizAyjOExJWC0NBZhZBjDM6ISEJGjROQJEXlaRJ4Vkc8H7SeLyI9FZJOIfEtEOoL2I4P3m4LPp2T8NxhGJBZhZBjDE2clsBc4R1V/G5gBvEdEzgJuBe5Q1d8EXgM+ERz/CeC1oP2O4DjDaAoWYWQYwzOiEtAKvwretgc/CpwDfCdovxeYH7y+NHhP8Pm5IiJpCWwYLsyfOYEl7zuNCV2dCDChq5Ml7zvNnMKGERArOkhE2oBe4DeBvwU2A7tV9UBwyFZg8KmaAGwBUNUDIvI6cDywK0W5DSM2tlWiYUQTSwmo6gAwQ0S6gO8Dpyb9YhFZCCwEmDx5ctLLGYZhJKKs+SRO0UGquht4FPjvQJeIDCqRiUBf8LoPmAQQfH4M8GrIte5W1W5V7R47dmxj0huGYaTAYD5J3+5+lMP5JCvX9414btGJEx00NlgBICKdwHnAT6gogw8Eh10JrAperw7eE3y+RlU1RZkNwzBSpcz5JHHMQeOAewO/wChghareLyLPAd8Ukb8E1gNfC47/GvBPIrIJ+AVweQZyG0ZLUFYThG+UOZ9kRCWgqhuAmSHtLwJnhrS/CVyWinSG0cJYSQt/GN/VSV/IgF+GfBLLGDaMJlFmE4RvlDmfxArIGUaTKLMJwjfSqlhbREwJGEaTKLMJwkfKmk9i5iDDaBKlNEFsWAF3TIdbuiq/N6xotkSlx1YChtEkSmeC2LACfvAp2B+sfl7fUnkPcPqC5slVcsSHEP7u7m7t6elpthiGYWTJHdMrA389x0yCz2zMX54WQER6VbU7yTXMHGQYRi7o61ud2o18MCVgGEYuvMIYp3YjH0wJGIaRC0v2XcYe7ahp26MdLNlnuaXNxJSAYRi50HP0eSzefxVbD47hoApbD45h8f6r6Dn6vGaLVmosOsgwjFxYNO8UbvzePlbvm3uorbO9jSWtHBJbAEwJGIaRC6ULiS0IpgQMw8iNsmbl+oz5BAzDMEqMKQHDMMqHla84hJmDDKOJ2KYyTcDKV9RgKwHDaBJl3te2qTzy54cVwCD7+yvtJcRWAoaRIi4z++E2lUm6GrAVxjBElakoafkKUwKGkRKu20VmtamMbVs5AsdMjChkNzF/WTzAzEFG65OTE9B1u8iozWOSbipj21aOwLk3Q3tdH7d3VtpLiCkBo7UZdAK+vgXQw07ADBSB68w+q01lbNvKETh9AVx8V6WENVL5ffFdpXQKg5mDDM9Zt/qrTHpyGSfoTnbIWLacsYjZl1wd/wLDOQFTfuhdt4vMKoM2lW0rN6yo9NHrWytmknNvbq1B8vQFrfX3JMCUgOEt61Z/lem9N9Ep+0DgJHZyTO9NrIP4iiBHJ2ClNs4zNaaYkWb2WWTQNiJHDRZCWSrMHGR4y6Qnl1UUQBWdso9JTy6Lf5EoZ18GTsD5Myew5H2nMaGrEwEmdHWy5H2n5e6MTSyHYwjlyvV9zFm6hpMXP8CcpWssxLVg2ErA8JYTdCdIWPuu+Bc59+baWS0M7wR0MINEhWH6EIGTSA6H1ZNFIhWfEVcCIjJJRB4VkedE5FkR+XTQfouI9InIU8HPBVXn3Cgim0TkeRGZl+UfYLQuO2RsRLvDTlQuTkAHJ3JLJ3o5rJ4sEqn4xDEHHQD+WFWnAWcB14jItOCzO1R1RvDzIEDw2eXAO4H3AF8WkbawCxvGcGw5YxH9dTtR9WsHW85Y5Hah0xdUNjK/ZXfld5Rd28EM0tKDn0MIpUUiFZ8RzUGquh3YHrz+pYj8BBhunXcp8E1V3Qu8JCKbgDOB/0pBXqNEzL7katZBEB20ix0yhi2zHKODXHAwg7T04DeoJGOYxVKJRGoCllF9GCefgIhMAWYCPwbmAH8kIlcAPVRWC69RURCPV522lRClISILgYUAkydPbkR2L7CbKVtmX3I1BIP+ScFPZjhkkhZ18ItNzBDKxJFITcD8GLXEjg4SkbcC3wWuU9U3gK8A7wBmUFkp/LXLF6vq3ararardY8eG2359p6Xtwq1OWBaxgxkkq0SvouFLRJQLLW3Ka4BYKwERaaeiAP5ZVb8HoKqvVH3+98D9wds+YFLV6RODtpYjywJgRoVMVlpRcfAX31X5iWEGsa0SD+NLRFRcWtqU1wAjKgEREeBrwE9U9faq9nGBvwDg94GNwevVwNdF5HZgPDAVeCJVqT3BbqZsyWzZPowDeOXZD7Fs711se7Of8Ud1smjgFOZHXKZog59RoeVNeY7EMQfNAf4AOKcuHPQ2EXlGRDYA7wY+A6CqzwIrgOeAfwGuUdWBiGsXmqwKgBkVMlu2RziA9fWtbuY9252qkJgpr5Y40UFrCU3Z4cFhzvkC8IUEchWCIjrFikRmK60IB/ArjIlv3rPSCoXFTHm1WMZwAuxmypbMlu0RWcRLfn1Z6OGhSifHwnRG+pgp7zCmBBJiN1N2ZLbSioiD73lwDMRVOrY7VbFp9SqpDpgSMLwl05VWSBz8ooG++ErHdqcqLmbKq8GUgOE1ea60nJSOa2E6wx/MlFeDKQHDqCK20nEorWB4hpnyajAlYBiNYrtTFRMz5dVgm8oYfmOx+Eba2EbzNdhKwPCH+oiNqefD0183B56RLmbKq0FUtdky0N3drT09Pc0Ww2gm9REbQCVHMeT+PGZSZV8Awyg5ItKrqt1JrmHmIMMPwiI2whQAlNaBZ2RMSU2PZg4y/MBlYC+pA8/IkBLnDpgSMDIldinoqIiNepNQiR14toFRetT35cNyM6NLmjtg5iAjM5w23YmK2Oj+eLxN4lsc28AoPcL68qg9L4cfXALToykBIzOcSkGfvqAywNcP+JPPykdYz7HdsNIjrC+36fHhB5fA9GhKwMgM51LQpy+oRP3csvtw9M8PPhWYifSwnbYkDrtqbAOj9Ajrs9sOLGCPdtQ2lsT0aD6BFsBXW3HiUtAZ1njxtc+iKOJuWL72cVhfrj44l+PaO7jlLd8tXe6ArQQKjs+24sQ7OGVU48XnPouiaLth+dzHUX0548KFtSvREigAMCVQeHy2Fc+fOYEl7zuNCV2dCDChq5Ml7zst/mwwyh6b0E7rc59Fkbgvc8bnPi5aX2aNmYMKju+24kSloDMq1+x7n0VRpA2MfO/jIvVl1thKoOC09Gb3URFDCZfpLd1nnmB9XBxMCRScotmKnamPGErBTtvyfeYBTenjkpZ9SIqZg2Lia6SDbXbvjvVZ9uTexyUu+5AUqyIag8FIh/q9Z8vsTDIyxOdN0H2V7Y7pERvFtHbFWasimhM+RzoYLcbgjNbHBDmfZbMtIxtmRCUgIpNE5FEReU5EnhWRTwftx4nIwyLyQvD72KBdROQuEdkkIhtE5Iys/4is8T3SIU9Wru9jztI1nLz4AeYsXeNF3HdLMVyCXLPxWbaMwonLQJyVwAHgj1V1GnAWcI2ITAMWA4+o6lTgkeA9wHuBqcHPQuArqUtNvoORRTpU8DkBqGUYbkbbbMenz7Nt2zKyYUZUAqq6XVWfDF7/EvgJMAG4FLg3OOxeYH7w+lLgPq3wONAlIuPSFDrvwciiSSqYWSwHomauncc23xTjyWw7dAKYUThxGXCKDhKRKcBM4MfAiaq6PfjoZeDE4PUEoNpDszVo205KDDcYZeGotWiSCpmbxcKcjuCnIzIj1r3jWqb33kSn7DvU1q8djDowwJHNrnefUfKeC/VBGoMTQID5Mxfk1he+Rgs2QmwlICJvBb4LXKeqb4jIoc9UVUXEKcxIRBZSMRcxefJkl1ObYqO3DMOMi5iFhfitugZU4eD+w20tHvZ33XNTmbX/Kq4/YgXj5VW26fHcdmABd0qEVTVPU4wHG7TnPQEMY3hFVLwxIpYSEJF2Kgrgn1X1e0HzKyIyTlW3B+aeHUF7HzCp6vSJQVsNqno3cDdUQkRdhC5iRcVWYNG8U0JDZVMxi4U5HQf2DT1uuNmvr+GLDmzb3U8fc1m9b25N+/UHVzBx1K6hJ+Tt+Dw9v9l2GD4EafigiNIkTnSQAF8DfqKqt1d9tBq4Mnh9JbCqqv2KIEroLOD1KrNRKpiNvjlkWnjLZUYbdqzP4YsORE1klnd8xByf+BGk4YMiSpM4K4E5wB8Az4jIU0HbnwBLgRUi8gng58Dg9OBB4AJgE7AH+FiaAoPZ6JtJZmaxyD2GI46tJ8O9BzKlbvVy57RruWLdbwxZbc24cCG0vXPISmflwByWLV1Tmucg09VoTFrNEmEZw4Yf1PsEANo6an0CUJn9hkV93NJFzYb0h5BK3SEfCfub2ztZd9rnue65qSMO7GXNZI9yyublrPWp39PIGLbaQYYfRDkdw9rCZvZRKwmfk4UiVi+zN3+RxxaPXOqg1WzTcQlbjebprG01S4QpAcMfQpyOK9f3sWzvXWx7s5/xR3WyaOCUQwkpNXgQvuhMwuSrVrNNJ6EZYeNFHfTrsdpBhrc4JQUWMVkoYfKVD05SXzCF2DimBAxvcc5QzmDvgUxJWOrAOUqu2WUnMsQUYuOYOcjwlpaf3UX4QeJG/Axrm67PmZh6Pjz99Zatt+9D1FBRseggw1vmLF0TGoo3oauTxy7YVfjEsDBSiTwJizpCCI2eaqF6+4Ur5ZBCcqNFBxnOFOlBiZrd3TntBfjB53Kb1ebZZ6k4OMOijkLDZ4mfm9EEXPu9UM5aj3ZCMyVQIopW8yTK3DH7R/87t8SwvPssFROYS/a1tHk5MSjaveqMR8mNpgRKRBHjykNnd6vyq2ufd5+lko3qkH2tOuDlYFvEe9UJj/ZmsOigEtEUR2sWESk51rXPu89SqYsVFnWEhB76CmO93COi5YMCPNmbAUwJlIrcw+iyKuqW4y5SefdZKkX6wnImuj8e2mdL9l0WeolmD7YtH/Lp0U5oZg4qEbmH0WVl98yxrn0zQg+dHJxRESZhJZ8nnzXk2J4Hx4CHxdBaPuTTg70ZBrEQ0ZKRqxPQsaibjw5Kn+WKKkDnkintUzG0erztd/Bm74o0QkRNCTQZr2/0pNwxPaKo29DYdJ8HI29x6N/h8PYe9GSgDZUrofJNC8sTKDgtHwbnUNQtlWgQXweNrHCNMInoHy/j6z2Kox+CR+GdaWCO4SbiXBunaDgUdUscDdIiO4sNS32kVeex4ceFRZgUrX+GG2ibjUfhnWlgK4Em0vJhcBB7T9rE8fFFnZ3FXb2EzYzbOmBU+9BNd8IiTHzvn/p+iMpz8GGgLeLeFcNgK4Em0vJhcA4kjo8v4uzMZXYeNogP7IMj3xZrpaUR/RDVnith/RCR1+DFQOtReGcamBJoIqkkBrniaTnhxPHxHiXfxMbF5BE1WPe/Fqt89iuMcWrPlchaR3WKwJeBtoh7VwyDmYOaSO7b1Lk629JwtDpcI5GDstV3Fktogliy7zKWtC9ntOw71LZHO1iy/zL+JtYVMiRyNaKVAdZHR39MM2cRMCUQl4wiT3KNzHCxC6cRnZFnhIcvyTcu94nLwJ5QyfUcfR6L34Drj1jBeHmVbXo8tx1YQO/R58U6P1Mi+6F1ylz7jCmBOPgcruaCy8wzDUdi3s7IZs/OXO+TqIF96vlBDkCIImlQyVUycPexet/cQ22d7W0s8SED11XBuSjasoUNN4ApgTj4HlkRF5eZZxqO1iI6a5Pgep+EDewj7QDW4P2Wu+nRBRcF56JoW2XyljGmBOLQKoOZy4wrjTC4FgulG5FG7pP6gf2O6ZlNOHJPCnOZhcdVcC6KtlUmbxkzYnSQiPyDiOwQkY1VbbeISJ+IPBX8XFD12Y0isklEnheReVkJnitFjDwJwyWqIY0wuBYLpRuRNO4TzyccK9f3MWfpGk5e/ABzlq5h5fq+8AOzSk5z6R/P+9IX4oSI3gO8J6T9DlWdEfw8CCAi04DLgXcG53xZRNpCzi0Www1mnoZcRnL6glghhamEwbVYKN2IpKH0PJ5wDJY56dvdj3K4zEmoIsgq49elfzzuS58Y0Rykqv8hIlNiXu9S4Juquhd4SUQ2AWcC/9W4iB4QZbOE1rY5puFobbazNk/SiFDyONTVqb5TVrNwl/7xuC99IolP4I9E5AqgB/hjVX0NmAA8XnXM1qCt+IQNZhnab42CklTp+RLqGoJTmZOs/EEu/eNxX/pEo0rgK8BfUEnr+wvgr4GPu1xARBYCCwEmT57coBhNxmyORolwqu809Xzo+Vp4e1JcFG2ZVqIN0lDZCFV9RVUHVPUg8PdUTD4AfcCkqkMnBm1h17hbVbtVtXvs2LGNiNF80rA5Fs2nYGSLx9U+ncqcvPCv4ReJajeaRkNKQETGVb39fWAwcmg1cLmIHCkiJwNTgSeSiegxSR2BHj/wRpPwuISyU30nWyUXhhHNQSLyDeBsYIyIbAU+B5wtIjOomIN+BlwNoKrPisgK4DngAHCNqg6EXLY1SGpztDhmox7PB8/YuQZlyxEpMHGigz4U0hxi7Dt0/BeALyQRKg7ebImXxObo+QNvNIFWGTwtMqcwFLKUtFO8ss9YHLNRT6sk2JUtR6TAFLJsRCr70fpAEWdLVpArW1oprNEicwpBIZVAy2zLWLQH3gpy5YMNnkaOFFIJjO/qZNYbDwe10XexTcf4UxvdlSI98ObINhzwxm9nDEshlcCd015geu9yOoNdkibKLm5tX87GaVOAc5oqW0tjjuz0uf+z0HsP6ABIG8z6KFx0e7OlciJssAe48XvPHDLbDvrtAFMEnlFIx/DszV88pAAG6ZR9zN78xSZJVBLMkZ0u93+2klU7GEWtA5X393+2uXI5EBWkccvqZyP9doZfFFIJ2Iy0SbRK5Iov9N7j1u4hUUEau/v3hx5fOL9dCSikOahlYqmLRtEc2WmRVURUVB5lRLuPNvZtu/u5ZNTaIf651Qfnhh4fWmfIaCrFVAJFDK1sFYrkyE6DLCOipC18wA/ZgmPQ7OKbjf3Ktz7B9fuXM7rKP7e0fTlvkSNYOTCnZpUQWWfIaCrFNAdZIoqRF1nW8pn10djtw+XGNJPr2791SAEMMlr28Wed345fZ8hoKsVcCUD5ZqRGc8jS/zQYBRQjOijT3JgE5q7R/dsj23Pf09hoiOIqActcNfIga//TRbfHCgl1quUfQahPoe2xZOYuB5OW4SfFNAdZCWYjLzyJiHKq5R9CVCjnnh/enMzc5ejcNvyjmErAp5rrWW0K43Jd25gmOzzxPznV8g8hyqdwVP/L4SfENXd1HufWbnhHMc1BvuQJZBU54nJdq+eTPWn4n1IwXyaxsUf6FA4ez8RRu4Z+YOHWpaGYKwFfMlezWpG4XDcFGVau72PO0jWcvPgB5ixdU7yS3L4zjPkyr76P8h0s7/hIMnNX/2tu7YZ3FFMJeGKnzWxF4nLdyGO3xDIRtczeDD4Toaj3/PBmt75PYPaL8inMuHBhMnOXLxMyo2GKaQ7yJXM1q8gRl+tGHYscbh/GRNQyezPgZ0YtEKmoj+p/OX7fJzT7DV4vvH8SmLsscbPwFFMJgB95Alk9AC7XDTsWobL9cxURJZ9bZW8GXzNqgUhFve3g8aGHh/Z9CmW8nXwKcX0YvkzIjIYprhLwgaweAJfrhh0bujIgdEaaRvy5D3i9oolQ6sv1I7Bv6OGhfZ9nMITrqiOFCZm3q7gSYEogKVmtSFyuW3/sHdNjm5MWzTulZgYNxazx4vWKJkKpzxiYQ2fcvs+zaGLOmwd5vYorAaYEWhEHc9LwtuLk5DXD835FE6LU5we/Y/XPuTdzYNW1HDHw5qGmA21HcUQWtvecQ7C9XsWVgHIrgVYtPeFopsqqxkueM7yirmji9v3KgTms3X8V1/FNxsurbNPjufPg5cwdmHNImTRM/XPQeSz0/2LocRlF/Hi9iisB5VUCrZ5k5YHjPM8ZXtYrmmaz7KHn6dv3u3yH361p/6+kfRn2HIxqh7YOGKhyWGQY8eP9Kq7FKa8SsE3TMyfvGV4rV63MrC/DnoOD+ytlHzrekvoqOcw8WNRVXKswYrKYiPyDiOwQkY1VbceJyMMi8kLw+9igXUTkLhHZJCIbROSMLIVPhC+lJ1qYqJlcS8/wXBO6Yh6fWV9G3e/9r8FnNsItuyu/U1IAYclxgO090ETirATuAb4E3FfVthh4RFWXisji4P0NwHuBqcHP7wBfCX57x57Ok0Jroe/pPInRIcdbCFuAgx/FdYZX+D52NTE6HJ/ZbDnHqKPhzIOPLT6nWP/rFmLElYCq/gdQ7yW6FLg3eH0vhwMdLgXu0wqPA10iMi4lWVPltv0fZI921LTt0Q5u2//BIcdaaYUAxxLeLpUvnfvYx8qprnWchiknUV9PKGkV0UhSKMESt/6ROYD9pFGfwImqOjiNfhk4MXg9AaieVmwN2oZMuUVkIbAQYPLkyQ2K0Tj3/upMfjFqX7BBdiXa4rYDC/jB3jO5pe7YIoawZTKrbsCPEtdO79THvjr1XU2MUeUk9rxM397K31YfUZX6/ZYw4dElAszZAdyq0XuekdgxrKoqIjrykUPOuxu4G6C7u9v5/KSM7+pk9e65rN43t6Z9QlfnkAE07MYFf2cwmYVmZuhHcZol+urUdzWtRJWT0NpyEoeUYdtj2QyKCSLJXJS3k0nLV0XfgjRaRfSVQTNP8HtH0N4HTKo6bmLQ5h1RVRXfferYIWYJibiGrw5O503J45pWMqwY6eT49NWp72paCTl+j3Zw24Ghg1z3Gw9zYNW1Naa4A6uubboZzEV5O5m0fNo4qsVpVAmsBq4MXl8JrKpqvyKIEjoLeL3KbOQVUTfkoz/dyXkD/87ajk/x4pH/k7Udn+LiUWuHKAKfQ9icZtUOdv5177iW/jo/Sr92sO4d1yaW2Wn7RF/LF7vuQhZy/G3tnwSouf8uGbWWGzpW1GQLAxwx8GZle8gm4hq1NH/mBB5bfA4vLb1weGewr4q+BRnRHCQi3wDOBsaIyFbgc8BSYIWIfAL4OTB4lz8IXABsAvYAH8tA5tQIs7E++u0vsaR9OaOlkigzUXaxtH057Ifeo88rROSKk+3VwbRy3XNTmbX/qiF+lN7npvLYJclkdkr2iiqLMfX8oG5SE23IrqaVuuMvXP1VpvfeRGfV/Xdr+3KODKs0B9HbQ+ZEVlFLrtF7RuOMqARU9UMRH50bcqwC1yQVqpnc2PFtRtc9cKNlH3/S8W1OWrykSVK54fRgOsy4tu3up4+hfhRJyTcS2/EZ5sycej6s/6fDWa6vb4FV19Qe30TiOupnb/4iSO391yn7OKCjGCUHhxy/7eDxNHP945ypff9nofeeykb00gazPgoX3T7ksNv2f5Dr9cuHJmNwOHrvFgf5Ch92nAPlzRiO4ERC9lsdpt1HnB5MB2emV+n99TPuW0+uLXMAlfc/vKHpSsDJUR+hlNvkIHu0Y8iguLzjI06DYhbEVt73fxZ6vnb4vQ4cfl+nCFyi96Kw6qTxMCVQh0QMitJse7MjsR9Mh4qjPqX318/w1r75i3AHflghtJxxCn+NuP/6O8dx86/fz3VaVUCOy5l74cIsRU+X3nui2+uUwHDRe3EpYmh3MzAlUE/ZtstziBP3pUhb2AyPIyEyjKvJDOeor1dmnz7hY1y8e+khnwBUnO/P/rfPMHfSRXzwoXOLa9rQgdjtaUw4LDktHqYE6injdnkOzkwfirSFzfB+oW/lePnV0IM7j8tOkJjJTFFmtK7R7UOU2Q27T2XtqCjne/P7PhHSFq4IpG1IUxoTjvFdncx64+GgL3exTcdU+vLo8xr+E1oRUwJheFCG2YgmbCb3+QNXsKz9qxwpVYPMqHZ4763ZCJFC3R9VhigzBVYfzM753lRmfbTWJ1DdHkLSCced015geu/yIZFWG6dNAc5p+LqtRqN5AobRNMIc0asPzmVJ+7W1Mfrzv5ydMndIZorKSXm9f3/sr/M1MdGJi26H7k8cnvlLW+V9SHRQGsze/MUasxpUIq1mb/5iJt9XVGwlYPhNiMll0bw5oTPrGRcuhJmfz0cux2SmsFntsoeeDzUTCZUVwSA+JyY6c9HtmQ36Q7CEs1jYSsDIn7hlKjasCC2VML/tsebXn08hazkqS/rDZ0222vpp4GtmuWfYSsDIFwdb+p4f3szoiFIJ82/4aXMHxhSiyHyJtmpZyhbp1yCmBJJi5W7dcChTEVUSIao91+zQlKLIfIi2allOX8C6n73GpCeXcYLuYoeMYctpi5htz2cNpgSSYOVu3XEpU3HweCaOGpqpve3g8Wxf/dXg4d7JDhnLf0z+Qz730jvzzQ61KDKvWbm+jxvX/Qb9+//mUFvnujaWTOozxVuF+QTiEmbHLmC527i7QGWGg512ecdHQnd/WztqFtN7b+IkdjJK4CR2ctHPl3LewL/XHDts+Wyj5XEuqZ4zTX8WA0wJxCGq3HJYzR3wNvrAi20yHWruz7hwITfrQrYeHMNBFbYeHMPNupB30Tsk9G+0VOrM1GPZoY3hywCVBJ8zhr14FgPMHBSHqBl/VAakp9EHXtRScS5T8ckhpRJOWvml0BIR4+XVoW2tEF+fEnF9Jq1SeM2rgod1ePEsBpgSiEPEzF51gDe1Y0idl43vuJbZecnmwLbd/Vwyau2QNPof7J478slpkrBMxcurxnISO4ccW78tY0vF1yfEZWD3aYBKgk8FD+vxaZViSiAOEZUdX2Esf7X/skw2WcmCK9/6BNfvH7phznHtHcCFzRXOgS1nLOKYqo1XoKJ8/8+UTzJhR6eFW4Ywkn08rz2184zg8jkE16dVilT2gWku3d3d2tPT02wxoqmPAgJo7+TTv/4Yqw4OnUUL8NJS/wbVPbeeGrFb0zhG3/DTJkjUOOsORQcFoX9nLGL2JVc3W6zcWVcXJRXVDycvfoCoJ72zva1GQdRnLA8yoauTxxY3XnOnfjUy+N1lTIZLqy9EpFdVu5PIYiuBOETYsXseHAMZafMsZkyjI+Lro9p9ZvYlV0Mw2J0U/JSNddVbUQZRUsf03sQ6GKIIomaebSKhReyyKF3RKmamNPBplWJKIC4hduxFA+HaPOnDkpljzmEXMcN/Jj25LLRA2qQnlx1SkINE2cfrB+VBlMrMP80Byic7uA/4kihoSiABWWnzzGZMjmn0Pu/P6rNseXGC7gyNkjpBhybYRd2rUUXskpp+wvDJDm4cxpRAQrLQ5pnNmBzCM30OE/RZtjzZIeFRUjtkTKh5LOpezSuCxudonTJjyWIeEjUzSmXGdPoC+MxGuGV35XdEqKbP2ZY+y5YnW85YRH9dRnW/drDljEWxrxG110EWyjTP7zLiYysBD/FhxuSz/dZn2fJk9iVXsw5qo6RmuUdJ5Wmb9sUObhzGlICH+BA54LP91mfZ8saipIykJFICIvIz4JfAAHBAVbtF5DjgW8AU4GfAAlV9LZmY5aPZMyYfViNR+CybYRSNNFYC71atCUdYDDyiqktFZHHw/oYUvsfIER9WI1H4LJtFLRlFI1HGcLAS6K5WAiLyPHC2qm4XkXHAj1R12Cma9xnDRiHIewCu/753nzqW7/b2WUaskRtpZAwnjQ5S4F9FpFdEFgZtJ6rqYG2Cl4ETE36HYYxI3qV5w77vnx//fxa1ZBSOpEpgrqqeAbwXuEZE3lX9oVaWGaFLDRFZKCI9ItKzc+fQWGfDcCHvsNGw74taU5ctaskoFol8AqraF/zeISLfB84EXhGRcVXmoB0R594N3A0Vc1ASOZqJ2YD9IO+wUZfrDhe1ZPeP0WwaXgmIyFtE5G2Dr4HzgY3AauDK4LArgVVJhfQVn3YHKjuZJtg5XLe+isNwUUt2/xg+kMQcdCKwVkSeBp4AHlDVfwGWAueJyAvA7wXvWxLLXPWHd5861qk9KYvmnUJne1tNW2d7Gx8+a3LsjFi7fwwfaNgcpKovAr8d0v4qcG4SoYqCZa42RhYmkEd/Gu5XimpPShphqnb/GD5gGcMJsMxVd7Iq/taMATVpQp/dP4YPWAG5BESZBCxzNZqsTCB5+wTSwO4fwwdMCSTAqiK6k9WMvYgDqt0/hg+YOSghza7xUzSyMoH4XEpiOOz+MZqNKQEjV7Is/mYDqmG4Y0rAyJWiztgNo1UxJWDkjs3YDcMfzDFsGIZRYkwJGIZhlBhTAoZhGCXGlIBhGEaJMSVgGIZRYhJtL5maECI7gZ/n8FVjgF0jHtUcfJXN5HLD5HLD5HKjXq7fUNVEpXK9UAJ5ISI9SffjzApfZTO53DC53DC53MhCLjMHGYZhlBhTAoZhGCWmbErg7mYLMAy+ymZyuWFyuWFyuZG6XKXyCRiGYRi1lG0lYBiGYVTREkpARI4TkYdF5IXg97ERx/2LiOwWkfvr2k8WkR+LyCYR+ZaIdATtRwbvNwWfT8lIriuDY14QkSuDtreJyFNVP7tE5M7gs4+KyM6qz67KS66g/Uci8nzV958QtDezv0aLyAMi8lMReVZEllYd31B/ich7gr9zk4gsDvk88u8VkRuD9udFZF7ca2Ypl4icJyK9IvJM8PucqnNC/6c5yTVFRPqrvvvvqs6ZFci7SUTuEhHJUa4P1z2DB0VkRvBZ4v6KKdu7RORJETkgIh+o+yzq+XTrM1Ut/A9wG7A4eL0YuDXiuHOBi4H769pXAJcHr/8O+MPg9SeBvwteXw58K225gOOAF4Pfxwavjw05rhd4V/D6o8CXsuyv4eQCfgR0h5zTtP4CRgPvDo7pAP4TeG+j/QW0AZuBtwfXexqYFufvBaYFxx8JnBxcpy3ONTOWayYwPng9HeirOif0f5qTXFOAjRHXfQI4CxDgh4P/0zzkqjvmNGBzWv3lINsU4HTgPuADMZ9Ppz5riZUAcClwb/D6XmB+2EGq+gjwy+q2QEueA3wn5Pzq634HONdxJhJHrnnAw6r6C1V9DXgYeE+djL8FnEBlYEuDVOQa4bq59peq7lHVRwFUdR/wJDDR4bvrORPYpKovBtf7ZiBflLzVf++lwDdVda+qvgRsCq4X55qZyaWq61V1W9D+LNApIkc6fn/qckVdUETGAUer6uNaGd3uI+LZzkGuDwXnpsmIsqnqz1R1A3Cw7tzQ56CRPmsVJXCiqm4PXr8MnOhw7vHAblU9ELzfCgwWu58AbAEIPn89OD5NuQ59R8j3DzI4O6n24r9fRDaIyHdEZJKDTGnJ9Y/BMvjPqh4YL/pLRLqorPgeqWp27a84/5eovzfq3DjXzFKuat4PPKmqe6vawv6necl1soisF5F/F5H/UXX81hGumbVcg3wQ+EZdW5L+iiub67nOfVaYTWVE5N+Ak0I++tPqN6qqIpJbyFNOcl0O/EHV+x8A31DVvSJyNZVZzDnVJ2Qs14dVtU9E3gZ8N5DtvjgnZt1fInIElYf1LlV9MWgesb/KhIi8E7gVOL+queH/aQpsByar6qsiMgtYGcjoBSLyO8AeVd1Y1dzM/kqVwigBVf29qM9E5BURGaeq24Pl0A6HS78KdInIEcEsYCLQF3zWB0wCtgaDyzHB8WnK1QecXfV+IhV74+A1fhs4QlV7q76zWoblVGzpNWQpl6r2Bb9/KSJfp7KsvQ8P+otKHPULqnpn1XeO2F8R31O9Yqi+L+qPqf97hzt3pGtmKRciMhH4PnCFqm4ePGGY/2nmcgUr3L3B9/eKyGbgt4Ljq016ufdXwOXUrQJS6K+4sg137tl15/6IBvqsVcxBq4FB7/iVwKq4JwY34KPAoOe9+vzq634AWFNnkklDroeA80XkWKlEw5wftA3yIepuwGCAHOQS4CcOMiWSS0SOEJExgRztwEXA4Aypqf0lIn9J5QG+rvqEBvtrHTBVKpFjHVQGgtXDyFv9964GLpdK1MnJwFQqzro418xMrsBM9gAV5/tjgweP8D/NQ66xItIWfP/bqfTXi4Fp8A0ROSswt1yBw7OdVK5AnlHAAqr8ASn1V1zZogh9Dhrqs5E82EX4oWK/ewR4Afg34LigvRtYXnXcfwI7gX4qtrJ5QfvbqTykm4BvA0cG7UcF7zcFn789I7k+HnzHJuBjddd4ETi1rm0JFcfe01QU2Kl5yQW8hUqk0oZAhr8B2prdX1RmPEplgH8q+LkqSX8BFwD/l0oEx58GbX8OXDLS30vFvLUZeJ6q6IywazZwvzckF3AT8Ouq/nmKSsBB5P80J7neH3zvU1Qc+hdXXbObygC7GfgSQYJrHnIFn50NPF53vVT6K6Zss6mMVb+msjp5dqRxw7XPLGPYMAyjxLSKOcgwDMNoAFMChmEYJcaUgGEYRokxJWAYhlFiTAkYhmGUGFMChmEYJcaUgGEYRokxJWAYhlFi/j/Di0vZ2G6fGQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고록:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression을 사용하여 분석을 위해 필요한 데이터들을 전처리 해보고 이를 통해 미래를 예측해보는 두번째 Explation이었다.  \n",
    "수학적인 개념을 적용시킨다는 것 자체가 생소하게 느껴져 코드 작성 초기에는 막막함이 앞섰다.  \n",
    "LMS에서 기능과 코드들을 친절하게 설명해주었기에 코드작성자체는 어렵지 않았지만, 코드를 실행 후 학습률과 테스트 데이터의 결과물들에서 적절한 값을 뽑아내는 기준을 선정하기 어려웠다.   \n",
    "특히 초반에 학습률을 최대한 작게 잡았을 때는 MSE 손실함수 값이 2만까지 넘어가 놀라기도 했었다.  \n",
    "이를 보완하기 계속해서 random_state등을 Learning_rate나 반복횟수등을 변경해보는 둥의 노력이 필요했다.   \n",
    "다행히도 반복하던 와중 프로젝트가 요구하는 정확도를 뽑아낼 수 있었다.  \n",
    "자전거 대여량을 예측하는 두번째 프로젝트는 시간을 분리하여 새로운 컬럼을 생성하는 부분에서부터 어려움이 느껴졌다.   \n",
    "다행히도 datetime을 사용해하여 분리하는 방법에 관한 링크가 있어 여러번 코드를 쓰고 지우고를 반복해 원하는 결과를 뽑아낼 수 있었다.  \n",
    "그래프를 만드는 과정에서도 subplot을 이용하여 데이터개수를 표현하고자 했을 때 어떻게 만들어야 할지 감이 잡히지 않아서 어려웠었다.  \n",
    "\n",
    "추가적으로 linear 학습률과 loss함수에 대해 처음에는 어려웠지만 아이펠 진행중 노드와 풀잎스쿨에서 관련 개념들을 발표해보면서 이해할 수 있는 시간이 있어서 코드를 작성하는데 큰 도움이 되었다.   \n",
    "배운 지식들을 연결해보는 과정이 중요함을 다시 한번 깨달았다.  \n",
    "\n",
    "저번시간에 이어서 pandas와 numpy의 기능을 활용하고 있다. 이 부분은 지속적으로 study가 필요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cs231 - https://cs231n.github.io/python-numpy-tutorial/  \n",
    "khan academy - https://ko.khanacademy.org/math/linear-algebra  \n",
    "인공지능을 위한 선형대수 - https://www.edwith.org/ai251/joinLectures/195088  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a59dc1dcb280767e1541beb883352acbe24e77c028211ef192031d7df32c832b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
